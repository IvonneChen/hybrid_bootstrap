#+title: The Hybrid Bootstrap: A Drop-in Replacement for Dropout
#+author: Robert Kosar @@latex:\\@@ David W. Scott
#+options: toc:nil

#+begin_abstract
Regularization is an important component of predictive model building.
The hybrid bootstrap is a regularization technique that functions
similarly to dropout except that features are resampled from other
training points rather than replaced with zeros.  We show that the
hybrid bootstrap offers superior performance to dropout.  We also
present a sampling based technique to simplify hyperparameter choice.
Next, we provide an alternative sampling technique for convolutional
neural networks.  Finally, we demonstrate the efficacy of the hybrid
bootstrap on non-image tasks using tree-based models.  The code used
to generate this paper is available
[[http://github.com/r-kosar/hybrid_bootstrap][here]].
#+end_abstract

* Latex Headers 						   :noexport:
#+latex_header: \usepackage[margin=1in]{geometry}
#+latex_header: \usepackage{setspace}
#+latex_header: \doublespacing 
#+latex_header: \usepackage{glossaries}
#+latex_header: \usepackage[export]{adjustbox}
#+latex_header: \usepackage{array}
#+latex_header: \hypersetup{colorlinks = true, linkcolor = black}
#+latex_header: \makeglossaries

* Glossary :noexport:
#+latex_header_extra: \newacronym{relu}{ReLU}{rectified linear unit}
#+latex_header_extra: \newacronym{cnn}{CNN}{convolutional neural network}
#+latex_header_extra: \newacronym{sgd}{SGD}{stochastic gradient descent}
#+latex_header_extra: \newacronym{gpu}{GPU}{graphics processing unit}
#+latex_header_extra: \newacronym{wrn}{WRN}{wide residual network}
* Introduction
<<introduction>> The field of machine learning offers many potent
models for inference.  Unfortunately, simply optimizing how well these
models perform on a fixed training sample often leads to relatively
poor performance on new test data compared to models that fit the
training data less well.  Regularization schemes are used to constrain
the fitted model to improve performance on new data.

One popular regularization tactic is to corrupt the training data with
independently sampled noise.  This constrains the model to work on
data that is different from the original training data in a way that
does not change the correct inference.  Sietsma and Dow demonstrated
that adding Gaussian noise to the inputs improved the generalization
of neural networks cite:sietsma1991creating.  More recently,
Srivastava et al. showed that setting a random collection of layer
inputs of a neural network to zero for each training example greatly
improved model test performance cite:srivastava2014dropout.

Both of these types of stochastic regularization have been shown to be
roughly interpretable as types of weight penalization, similar to
traditional statistical shrinkage techniques.  Bishop showed that
using a small amount of additive noise is approximately a form of
generalized Tikhonov regularization cite:bishop1995training.  Van der
Maaten et al. showed that dropout and several other types of sampled
noise can be replaced in linear models with modified loss functions
that have the same effect cite:van2013learning. Similarly, Wager et
al. showed that, for generalized linear models, using dropout is
approximately equivalent to an $l^2$ penalty following a scaling of the
design matrix cite:wager2013dropout.

As noted by Goodfellow et al., corrupting noise can be viewed as a
form of dataset augmentation cite:goodfellow2016deep.  Traditional
data augmentation seeks to transform training points in ways that may
drastically alter the point but minimally change the correct
inference.  Corruption generally makes the correct inference more
ambiguous.  Often, effective data augmentation requires domain-specific
knowledge.  However, data augmentation also tends to be much more
effective than corruption, presumably because it prepares models for
data similar to that which they may actually encounter.  For example,
DropConnect is a stochastic corruption method that is similar to
dropout except that it randomly sets neural network weights, rather
than inputs, to zero.  Wan et al. showed that DropConnect (and dropout)
could be used to reduce the error of a neural network on the MNIST
cite:lecun1998gradient digits benchmark by roughly 20 percent.
However, using only traditional augmentation they were able to reduce
the error by roughly 70 percent cite:wan2013regularization.  Since
corruption seems to be a less effective regularizer than traditional
data augmentation, we improved dropout by modifying it to be more
closely related to the underlying data generation process.

** The Hybrid Bootstrap
   <<thehb>> An obvious criticism of dropout as a data augmentation
scheme is that one does not usually expect to encounter randomly
zeroed features in real data, except perhaps in the ``nightmare at test
time'' cite:globerson2006nightmare scenario where important features
are actually anticipated to be missing.  One therefore may wish to
replace some of the elements of a training point with values more
plausible than zeros.  A natural solution is to sample a replacement
from the other training points.  This guarantees that the replacement
arises from the correct joint distribution of the elements being
replaced.  We call this scheme the hybrid bootstrap because it
produces hybrids of the training points by bootstrap
cite:efron1994introduction sampling.  More formally, define
$\boldsymbol{x}$ to be a vectorized training point.  Then a dropout
sample point $\tilde{\boldsymbol{x}}$ of $\boldsymbol{x}$ is
#+name: dropout_def
\begin{equation}
\tilde{\boldsymbol{x}} = \frac{1}{1-p}\boldsymbol{x} \odot \boldsymbol{\epsilon},
\end{equation}
where $\odot$ is the elementwise product, $\boldsymbol{\epsilon}$ is a
random vector of appropriate dimension such that $\epsilon_i \sim
Ber(1 - p)$, and $p \in [0,1]$.  The normalization by $\frac{1}{1-p}$
seems to be in common use, although it was not part of the original
dropout specification cite:srivastava2014dropout.  We then define a
hybrid bootstrap sample point
$\boldsymbol{\overset{\overset{\large.}{\cup}}{x}}$ as
#+name: hb_def 
\begin{equation}
\boldsymbol{\overset{\overset{\large.}{\cup}}{x}} =  \boldsymbol{x} \odot \boldsymbol{\epsilon} 
+  \boldsymbol{\overset{\large.}{\cup}} \odot (\boldsymbol{1} - \boldsymbol{\epsilon}),
\end{equation}
where $\boldsymbol{\overset{\large.}{\cup}}$ is a vectorized random training
point and $\boldsymbol{1}$ is a vector of ones of appropriate
length.  In Figure [[hyb_drop_visual]], we compare dropout and hybrid
bootstrap sample points for a digit 5.
#+name: Get MNIST train images for R
#+begin_src python :exports none :cache yes
  import pandas as pd
  from keras.datasets import mnist

  (X_train, y_train), (X_test, y_test) = mnist.load_data()

  df = pd.DataFrame(X_train[0:2].reshape((2, -1))).T
  df.to_csv('./data/first_2_digits.csv')
#+end_src

#+RESULTS[463fd966c634e1314c7b36149da43c6f09d087e1]: Get
: None

#+name: Basic Hybrid Bootstrap Visualization
#+begin_src R :exports results :file basic_visual.pdf
  set.seed(42)
  library(png)
  library(grid)
  library(ggplot2)

  images <- read.csv("./data/first_2_digits")
  images <- lapply(images, function(x){matrix(x, 28, 28) / 255})[-1]

                                          # Simple hybrid bootstrap that expects a list of 2 images and a probability
                                          # returns hybrid bootstrap sample of first image
  hybrid_bootstrap <- function(x, p){
      x_dim <- dim(x[[1]])
      mask <- matrix(rbinom(x_dim[1] * x_dim[2], size = 1, prob = 1 - p), x_dim[1], x_dim[2])
      sample <- x[[1]] * mask + x[[2]] * (1 - mask)
      return(t(sample))}

                                          # Simple dropout sample of first image
  dropout <- function(x, p){
      x_dim <- dim(x[[1]])
      mask <- matrix(rbinom(x_dim[1] * x_dim[2], size = 1, prob = 1 - p), x_dim[1], x_dim[2])
      sample <- x[[1]] * mask
      return(t(sample))}

  num_p <- 5
  scale_factor <- 0.9
  ps <- seq(0, 1, length.out = 5)
  p_low <- (ps[1] - ps[2]) / 2 * scale_factor
  p_high <- 1 - p_low
  df <- data.frame(Method = factor(c("Hybrid Bootstrap", "Dropout"), levels = c("Hybrid Bootstrap", "Dropout")),
                   p = c(p_low, p_high))

  plot <- ggplot(df, aes(p, Method)) +
      scale_x_continuous(breaks = ps) + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
      geom_blank() +
      xlab(expression(italic(p)))


  for(i in 1:num_p){
      g <- rasterGrob(hybrid_bootstrap(images, ps[i]))
      plot <- plot + annotation_custom(g, xmin = ps[i] + p_low, xmax =ps[i] - p_low,
                                       ymin = 0.5, ymax = 1.5)
  }
  for(i in 1:num_p){
      g <- rasterGrob(dropout(images, ps[i]))
      plot <- plot + annotation_custom(g, xmin = ps[i] + p_low, xmax =ps[i] - p_low,
                                       ymin = 1.5, ymax = 2.5)
  }

  ggsave("basic_visual.pdf", plot, device = "pdf", width = 6.5,
         height = 3, units = "in")
#+end_src
#+caption: Dropout randomly (with probability $p$) sets a selection of covariates to zero.  
#+caption: The hybrid bootstrap randomly replaces a selection of covariates 
#+caption: with those from another training point.  The dropout samples in this figure have not 
#+caption: been normalized for their corruption levels as would typically be done 
#+caption: when used for training.
#+label: hyb_drop_visual
#+RESULTS: Basic
[[file:basic_visual.pdf]]

Typically dropout is performed with the normalization given in
Equation [[dropout_def]], but we do not use that normalization for this
figure because it would make the lightly corrupted images dim; we do
use the normalization elsewhere for dropout.  This normalization does
not seem to be useful for the hybrid bootstrap.  One clear difference
between the hybrid bootstrap and dropout for the image data of Figure
[[hyb_drop_visual]] is that the dropout corrupted sample point remains
recognizable even for corruption levels greater than 0.5, whereas the
hybrid bootstrap sample, unsurprisingly, appears to be more strongly
the corrupting digit 0 at such levels.  In general, we find that lower
fractions of covariates should be resampled for the hybrid bootstrap
than should be dropped in dropout.

** Paper outline
   <<outline>> In this paper, we focus on applying the hybrid
   bootstrap to image classification using glspl:cnn
   cite:lecun1989backpropagation in the same layerwise way dropout is
   typically incorporated. The basic hybrid bootstrap is an effective
   tool in its own right, but we have also developed several
   refinements that improve its performance both for general
   prediction purposes and particularly for image classification.  In
   Section [[choose_p]], we discuss a technique for simplifying the choice
   of the hyperparameter $p$ for the hybrid bootstrap and dropout.  In
   Section [[conv_sampling]], we introduce a sampling modification that
   improves the performance of the hybrid bootstrap when used with
   convolutional neural networks.  In Section [[perf_vs_size]], we compare
   the performance of the hybrid bootstrap and dropout for different
   amounts of training data.  Section [[benchmarks]] contains results on
   several standard benchmark image datasets.  The hybrid bootstrap is
   a useful addition to models besides convolutional neural networks;
   we present some performance results for the multilayer perceptron
   cite:rosenblatt1961principles and gradient boosted trees
   cite:friedman2001greedy in Section [[other_algorithms]].

* CNN Implementation Details
  <<implementation>> We fit the glspl:cnn in this paper using
  backpropagation and gls:sgd with momentum.  The hybrid bootstrap
  requires selection of $\boldsymbol{\overset{\large.}{\cup}}$ in
  Equation [[hb_def]] for each training example.  We find that using a
  $\boldsymbol{\overset{\large.}{\cup}}$ corresponding to the same
  training point to regularize every layer of a neural network leads
  to worse performance than using different points for each layer.
  We use a shifted version of each training minibatch to serve as the
  collection of $\boldsymbol{\overset{\large.}{\cup}}$ required to
  compute the input to each layer.  We increment the shift by one for
  each layer. In other words, the corrupting noise for the first
  element of a minibatch will come from the second element for the
  input layer, from the third element for the second layer, and so on.
  We use a simple network architecture given in
  Figure [[experiment_architecture]] for all gls:cnn examples except
  those in Section [[benchmarks]].  All activation functions except for
  the last layer are glspl:relu cite:nair2010rectified.  As Srivastava
  et al. cite:srivastava2014dropout found for dropout, we find that
  using large amounts of momentum is helpful for obtaining peak
  performance with the hybrid bootstrap and generally use a momentum
  of 0.99 by the end of our training schedules, except in
  Section [[benchmarks]].  We make extensive use of both Keras
  cite:chollet2015keras, and Theano cite:2016arXiv160502688short to
  implement neural networks and train them on glspl:gpu.

  #+name:experiment_architecture
  #+begin_src python :exports results :results file
    from graphviz import Digraph

    # It would be better if this was generated by the actual code using
    # a parser

    my_graph = Digraph()
    my_graph.attr(ranksep = '0.2')
    my_graph.attr(nodesep = '2')
    my_graph.attr('graph', size = '6.5, 6.5', margin = '0')
    my_graph.attr('node', shape = 'rect')
    my_graph.attr('edge', weight = '2')


    my_graph.node('0', 'Image', style = 'filled', fillcolor = 'grey')
    my_graph.node('1', 'Regularization Layer, Hyperparameter = (p or u) / 2', style = 'filled', fillcolor = 'forestgreen')
    my_graph.node('2', 'Basic Block', style = 'filled', fillcolor = 'white')
    my_graph.node('3', 'Regularization Layer, Hyperparameter = p or u', style = 'filled', fillcolor = 'forestgreen')
    my_graph.node('4', 'Basic Block', style = 'filled', fillcolor = 'white')
    my_graph.node('5', 'Regularization Layer, Hyperparameter = p or u', style = 'filled', fillcolor = 'forestgreen')
    my_graph.node('6', 'Softmax Layer', style = 'filled', fillcolor = 'blanchedalmond')
    my_graph.node('7', 'Class Probabilities', style = 'filled', fillcolor = 'grey')

    for i in range(7):
        my_graph.edge(str(i), str(i + 1))
        print([str(i) + str(i + 1)])


    my_graph.node('8', 'Basic Block', style = 'filled', fillcolor = 'white')
    my_graph.node('9', 'Convolution Layer, 32 5x5 Filters, No Padding', style = 'filled', fillcolor = 'blanchedalmond')
    my_graph.node('10', 'Regularization Layer, Hyperparameter = p or u', style = 'filled', fillcolor = 'forestgreen')
    my_graph.node('11', 'Convolution Layer, 32 5x5 Filters, No Padding', style = 'filled', fillcolor = 'blanchedalmond')
    my_graph.node('12', 'Max Pooling, Size 2x2, Stride 2, No Padding', style = 'filled', fillcolor = 'deepskyblue1')

    for i in range(9, 12):
        my_graph.edge(str(i), str(i + 1))
        print([str(i) + str(i + 1)])

    my_graph.edge(str(8), str(9), style = 'invis')
    my_graph.render('./example_network', view = False)
    return './example_network.pdf'

  #+end_src
  #+caption: Network architecture for gls:cnn experiments in this paper (except the benchmark results).  
  #+label: experiment_architecture
  #+attr_latex: :height 3.5in
  #+RESULTS: experiment_architecture
  [[file:./example_network.pdf]]

* Choosing $p$
  <<choose_p>>
#+name:basic_hb
#+begin_src python :exports none
  import numpy as np
  from keras import backend as K
  from theano import tensor as T
  from keras.engine.topology import Layer
  from keras.initializers import Constant


  def hybo(x, p, shift, seed = None, unif = True, just_dropout = False):
      '''Theano hybrid bootstrap backend'''
      if p.get_value() < 0. or p.get_value() > 1:
          raise Exception('Hybrid bootstrap p must be in interval [0, 1].')

      if seed is None:
          seed = np.random.randint(1, 10e6)
          rng = K.RandomStreams(seed = seed)

      if(unif == True):
          retain_prob = 1. - rng.uniform((x.shape[0],), 0, p, dtype = x.dtype)
          for dim in range(x.ndim - 1):
              retain_prob = K.expand_dims(retain_prob, dim + 1)
      else:
          retain_prob = 1. - p
          
      mask = rng.binomial(x.shape, p = retain_prob, dtype = x.dtype)
      if just_dropout:
          x = x * mask / retain_prob
      else:
          x = x * mask + (1 - mask) * T.roll(x, shift = shift, axis = 0)

      return x



  class HB(Layer):
      '''Applies the hybrid bootstrap to the input.
          # Arguments
          p: float between 0 and 1. Fraction of the input units to resample if unif = F,
             maximum fraction if unif = T
          shift: int. Should be smaller than batch size.
          unif: bool.  Should p be sampled from unif(0, p)?
          just_dropout: Should we just do dropout?
          '''
      def __init__(self, p, shift, unif = True, just_dropout = False, **kwargs):
          self.init_p = p
          self.shift = shift
          self.unif = unif
          self.just_dropout = just_dropout
          self.uses_learning_phase = True
          self.supports_masking = True
          super(HB, self).__init__(**kwargs)

      def build(self, input_shape):
          self.p = self.add_weight(shape=(),
                                   name = 'p',
                                   initializer=Constant(value = self.init_p),
                                   trainable=False)
          super(HB, self).build(input_shape)

      def call(self, x, mask=None):
          if 0. < self.p.get_value() <= 1.:
              x = K.in_train_phase(hybo(x, p = self.p, shift = self.shift,
                                        unif = self.unif,
                                        just_dropout = self.just_dropout), x)
          return x

      def get_config(self):
          config = {'init_p': self.init_p, 'p': self.p, 'shift': self.shift,
                    'unif': self.unif, 'just_dropout': self.just_dropout}
          base_config = super(HB, self).get_config()
          return dict(list(base_config.items()) + list(config.items()))



#+end_src

#+RESULTS: basic_hb
: None

#+name: Loss vs p
#+begin_src python :cache yes :noweb yes :exports none
  '''Based on https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py'''
  import numpy as np
  np.random.seed(42)
  import pandas as pd
  from keras.datasets import mnist
  from keras.models import Sequential
  from keras.layers import Dense, Dropout, Activation, Flatten
  from keras.layers.convolutional import Conv2D
  from keras.layers.pooling import MaxPooling2D
  from keras.utils import np_utils
  from keras.optimizers import SGD
  from sklearn.metrics import log_loss
  from keras.callbacks import Callback

  <<basic_hb>>

  class UncorruptedTrainHistory(Callback):
      '''Callback to keep track of uncorrupted training losses'''
      def __init__(self, data):
          self.data_X, self.data_Y = data
      
      def on_train_begin(self, logs={}):
          self.losses = []
          self.accs = []
      
      def on_epoch_end(self, batch, logs={}):
          loss, acc = self.model.evaluate(self.data_X, self.data_Y)
          print("\nUncorrupted Training Loss: {}, Uncorrupted Training Accuracy: {}" .format(loss, acc))
          self.losses.append(loss)
          self.accs.append(acc)

  class OptimizerScheduler(Callback):
      """Schedule for the optimizer.  Virtually identical to keras LearningRateScheduler but
      sets momentum (also I took out the chedule output checker)
      # Arguments
          schedule: a function that takes an epoch index as input
              (integer, indexed from 0) and returns a new
              learning rate as output (float).
      """
      def __init__(self, schedule):
          super(OptimizerScheduler, self).__init__()
          self.schedule = schedule

      def on_epoch_begin(self, epoch, logs=None):
          if not hasattr(self.model.optimizer, 'lr'):
              raise ValueError('Optimizer must have a "lr" attribute.')
          lr, momentum = self.schedule(epoch)
          K.set_value(self.model.optimizer.lr, lr)
          K.set_value(self.model.optimizer.momentum, momentum)
          

  batch_size = 128
  nb_classes = 10
  img_rows, img_cols = 28, 28
  nb_filters = 32
  nb_pool = 2
  nb_conv = 5

  (X_train, y_train), (X_test, y_test) = mnist.load_data()
  X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
  X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)
  X_train = X_train.astype('float32')
  X_test = X_test.astype('float32')
  X_train /= 255
  X_test /= 255
  Y_train = np_utils.to_categorical(y_train, nb_classes)
  Y_test = np_utils.to_categorical(y_test, nb_classes)

  small_indices = np.array([], dtype = 'int32')
  for i in range(nb_classes):
      first_100_i_indices = np.arange(X_train.shape[0])[y_train == i][0:100]
      small_indices = np.concatenate([small_indices, first_100_i_indices])
      
  small_X_train = X_train[small_indices]
  small_y_train = y_train[small_indices]
  small_Y_train = Y_train[small_indices]

  def schedule(x):
      '''Rather ugly way of defining a learning schedule function'''
      x = np.array(x, dtype = 'float32')
      lr = np.piecewise(x, [x < 500, (x >= 500) & (x < 1000), x >= 1000],
                          [0.01, 0.001, 0.0001])
      momentum = np.piecewise(x, [x < 500, x >= 500],
                          [0.9, 0.99])
      return((lr, momentum))

  frames = []
  uncorrupted_train_history = UncorruptedTrainHistory([small_X_train, small_Y_train])
  optimizer_scheduler = OptimizerScheduler(schedule)
  for just_dropout in [False, True]:
      for unif in [False, True]:
          for hbp in (np.arange(31) / 30.):
              model = Sequential()
              model.add(HB(hbp / 2, shift = -1, input_shape = (img_rows, img_cols, 1), unif = unif, just_dropout = just_dropout))
              model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
              model.add(HB(hbp, shift = -2, unif = unif, just_dropout = just_dropout))
              model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
              model.add(MaxPooling2D((nb_pool, nb_pool)))
              model.add(HB(hbp, shift = -3, unif = unif, just_dropout = just_dropout))
              model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
              model.add(HB(hbp, shift = -4, unif = unif, just_dropout = just_dropout))
              model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
              model.add(MaxPooling2D((2, 2)))
              model.add(Flatten())
              model.add(HB(hbp, shift = -6, unif = unif, just_dropout = just_dropout))
              model.add(Dense(nb_classes, activation = 'softmax'))
              sgd = SGD(lr = 0.01, momentum = 0.9, decay = 0.00, nesterov = False)
              model.compile(loss='categorical_crossentropy',
                            optimizer= sgd,
                            metrics=['accuracy'])
              myfit = model.fit(small_X_train, small_Y_train, batch_size = batch_size, epochs = 1500,
                                verbose = 1, validation_data=(X_test, Y_test), callbacks = [uncorrupted_train_history, optimizer_scheduler])
              df = pd.DataFrame.from_dict(myfit.history)
              df['uncorrupted_train_acc'] = uncorrupted_train_history.accs
              df['uncorrupted_train_loss'] = uncorrupted_train_history.losses
              df['hbp'] = np.repeat(hbp, 1500)
              df['uniform'] = np.repeat(unif, 1500)
              df['just_dropout'] = np.repeat(just_dropout, 1500)
              frames.append(df)

  out_frame = pd.concat(frames)
  out_frame.to_csv('./data/p_curves.csv')
#+end_src
#+RESULTS[56a57a04e0fe3ba07fa5e5329b84195fe6f95ee1]: Loss
: None

The basic hybrid bootstrap requires selection of a hyperparameter $p$
to determine what fraction of inputs are to be resampled.  This is a
nuisance because the quality of the selection can have a dramatic
effect on performance, and a lot of computational resources are
required to perform cross validation for complicated networks.
Fortunately, $p$ need not be a fixed value, and we find that sampling
$p$ for each hybrid bootstrap sample to be effective.  We sample from
a $\text{Uniform}(0, u)$ distribution.  Sampling in this way offers
two advantages relative to using a single value:
1. Performance is much less sensitive to the choice of $u$ than it is
   to the choice of $p$ (i.e. tuning is easier).
2. Occasionally employing near-zero levels of corruption ensures
   that the model performs well on the real training data.
#+name: Loss_vs_p_figure
#+begin_src R :exports results :file choosing_p.pdf
  library(dplyr)
  library(ggplot2)
  library(gridExtra)
  library(scales)
  library(grid)
  data <- read.csv("./data/p_curves.csv")
  data <- mutate(group_by(data, hbp, uniform),
                       end_error = 1 - val_acc[1500])
  data <- mutate(data,
                       match_p = (uniform == "False" & round(hbp * 30) == 3))
  data$match_p[data$uniform == "True" & round(data$hbp * 30) == 7] = 1
  data$match_p[data$uniform == "False" & round(data$hbp * 30) == 10] = 2
  data$match_p[data$uniform == "True" & round(data$hbp * 30) == 20] = 2
  data$match_p = as.character(data$match_p)
  data$uniform <- factor(data$uniform, 
                         levels = c("False", "True"),
                         labels = c(expression(italic(p)),
                                    expression(paste("Unif(0, ", italic(u),")" ))))
  no_dropout <- data[data$just_dropout == "False",]

  # Thin the curves so the plot is readable
  #no_dropout <- no_dropout[(no_dropout$hbp * 100) %% 10 == 0, ]


  plot1 <- ggplot(data = no_dropout) +
    geom_line(aes(x = X, y = 1 - val_acc, group = interaction(hbp, uniform),
                  linetype = uniform, color = match_p)) +
    scale_y_continuous(trans = "log2") + 
    scale_x_continuous(trans = "log2", limits = c(8, 1500)) +
    geom_vline(aes(xintercept = 500), linetype = 4) + 
    geom_vline(aes(xintercept = 1000), linetype = 4) + 
    annotate("label", x = 64, y = 0.9,
             label = "lr: 0.01 \n mom: 0.9", size = 1.9) + 
    annotate("label", x = 700, y = 0.9,
             label = "lr: 0.001 \n mom: 0.99", size = 1.9) + 
    annotate("label", x = 1400, y = 0.9,
             label = "lr: 0.0001 \n mom: 0.99", size = 1.9) + 
    guides(linetype = guide_legend(title = "Parameter", parse.label = TRUE, order = 1),
           color = FALSE) + 
    scale_color_manual(values = c("1" = muted("blue"), "2" = muted("red"), "0" = alpha("grey", alpha = 0.3))) +
    scale_linetype_manual(values = c(1 , 3), 
                          labels = lapply(attributes(data$uniform)$levels, function(x){parse(text = x)})) +
    ylab("Test Set Misclassification Rate") +
    xlab("Epochs") + 
      theme(legend.text = element_text(size = 8), legend.title = element_text(size = 8))

  plot1 <- plot1 +
      geom_line(data = no_dropout[no_dropout$match_p != 0,],
                aes(x = X, y = 1 - val_acc, group = interaction(hbp, uniform),
                    linetype = uniform, color = match_p)) +
      scale_color_manual(values = c("1" = muted("blue"), "2" = muted("red"), "0" = alpha("grey", alpha = 0.3)))


  final_values <- data[data$X == 1499,]

  best_values <- summarise(group_by(final_values, just_dropout, uniform),
                           max = max(val_acc))

  vbar_df <- data.frame(value = c(3 / 30, 7 / 30, 10 / 30, 20 / 30),
                        uniform = c("False",
                                    "True",
                                    "False",
                                    "True"),
                        color = c("left",
                                  "left",
                                  "right", 
                                  "right"))
  vbar_df$uniform <- factor(vbar_df$uniform, 
                         levels = c("False", "True"),
                         labels = c(expression(italic(p)),
                                    expression(paste("Unif(0, ", italic(u),")" ))))

  plot2 <- ggplot(final_values, aes(x = hbp,
                                    y = 1 - val_acc,
                                    color = just_dropout,
                                    group = interaction(hbp, uniform, just_dropout))) +
    geom_point() +
    geom_line(aes(group = interaction(just_dropout))) + 
    geom_hline(aes(yintercept = 1 - max, color = just_dropout), best_values) + 
    geom_vline(data = vbar_df , 
               aes(xintercept = value, color = color), show.legend = FALSE) + 
    facet_grid(. ~ uniform, labeller = "label_parsed") + 
    ylab("Test Set Misclassification Rate") +
    scale_color_manual(name = "Method",
                         breaks = c("True", "False"),
                         labels = c("Dropout", "Hybrid\nBootstrap"),
                         values = c(hcl(h = 15, c = 100, l = 65),
                                    muted("blue"),
                                    muted("red"),
                                    c(hcl(h = 195, c = 100, l = 65)))) +
    xlab("Regularization Parameter") +
    ylim(c(0, 0.1)) +
    theme(panel.spacing = unit(0.75, "lines")) +
    theme(legend.text = element_text(size = 8), legend.title = element_text(size = 8))


  figure <- grid.arrange(plot2, plot1, ncol = 1)
  ggsave("choosing_p.pdf", figure, device = "pdf", width = 6.5,
           height = 5.5, units = "in")
#+end_src
#+caption: Test set performance (top) and test set performance over the course of training (bottom) 
#+caption: using a  constant hyperparameter vs. a sampled hyperparameter for 
#+caption: dropout and the hybrid bootstrap on the MNIST digits with 1,000 training examples.
#+caption: The colored lines in the bottom panel correspond to the regularization levels indicated in the top panel.
#+caption: Sampled hyperparameters perform as well as constant hyperparameters but 
#+caption: are much less sensitive to the choice of $u$ than to $p$.
#+label: loss_vs_p
#+RESULTS: Loss_vs_p_figure
[[file:choosing_p.pdf]]
The first advantage is illustrated in the top panel of Figure
[[loss_vs_p]].  Clearly there are many satisfactory choices of $u$ for
both the hybrid bootstrap and dropout, whereas only a narrow range of
$p$ is nearly optimal.  However, as the bottom panel of Figure
[[loss_vs_p]] demonstrates, this insensitivity is somewhat contingent upon
training for a sufficient number of epochs. The advantages of sampling
a regularization level for each training point follow from the way
neural networks are fit.  gls:sgd and gls:sgd with momentum update the
parameters of a neural network by translating them in the direction of
the negative gradient of a minibatch or a moving average of the
negative gradient of minibatches respectively.  The minibatch gradient
can be written as
#+name: sgd_gradient
\begin{equation}
\frac{1}{m}\nabla_{\boldsymbol{\theta}}\sum_{i = 1}^mL(f(\boldsymbol{x}^{(i)};\boldsymbol{\theta}), \boldsymbol{y}^{(i)}),
\end{equation}
where $m$ is the number of points in the batch, $\boldsymbol{\theta}$ is
the vector of model parameters, $L$ is the loss, $f$ is the model,
$\boldsymbol{x}^{(i)}$ is the ith training example in the batch, and $\boldsymbol{y}^{(i)}$ is
the target of the ith training example in the batch cite:goodfellow2016deep. Equation [[sgd_gradient]]
can be rewritten using the chain rule as
#+name: chain_sgd_gradient
\begin{equation}
\frac{1}{m}\sum_{i = 1}^m \left[ \nabla_{f(\boldsymbol{x}^{(i)};\boldsymbol{\theta})}
L(f(\boldsymbol{x}^{(i)};\boldsymbol{\theta}), \boldsymbol{y}^{(i)}) \cdot 
\frac{Df(\boldsymbol{x}^{(i)};\boldsymbol{\theta})}{d\boldsymbol{\theta}} \right].
\end{equation}
The gradient of the loss in Equation [[chain_sgd_gradient]] is ``small''
when the loss is small; therefore, the individual contribution to the
minibatch gradient is small from individual training examples with
small losses.  As training progresses, the model tends to have
relatively small losses for relatively less-corrupted training points.
Therefore, less-corrupted examples contribute less to the gradient
after many epochs of training.  We illustrate this in Figure
[[gradient_figure]] by observing the Euclidean norm of the gradient in
each layer as training of our experimental architecture on 1,000 MNIST
training digits progresses.
#+name:compute_grads
#+begin_src python :cache yes :exports none
  """This script is just for computing gradients; it does not appropriately
  end the learning phase (but we don't need it to)"""

  import numpy as np
  np.random.seed(42)
  from keras import backend as K
  from theano import tensor as T
  from keras.engine.topology import Layer
  from keras.initializers import Constant
  import pandas as pd
  from keras.datasets import mnist
  from keras.models import Sequential
  from keras.layers import Dense, Dropout, Activation, Flatten
  from keras.layers.convolutional import Conv2D
  from keras.layers.pooling import MaxPooling2D
  from keras.utils import np_utils
  from keras.optimizers import SGD
  from sklearn.metrics import log_loss
  from keras.callbacks import Callback
  from keras.layers import Input


  # We do need to redefine HB so we can't use noweb here
  def hybo(x, p, shift, seed = None, unif = True, just_dropout = False):
    '''Theano hybrid bootstrap backend'''
    if p.get_value() < 0. or p.get_value() > 1:
        raise Exception('Hybrid bootstrap p must be in interval [0, 1].')

    if seed is None:
        seed = np.random.randint(1, 10e6)
        rng = K.RandomStreams(seed = seed)

    if(unif == True):
        retain_prob = 1. - rng.uniform((x.shape[0],), 0, p, dtype = x.dtype)
        for dim in range(x.ndim - 1):
            retain_prob = K.expand_dims(retain_prob, dim + 1)
    else:
        retain_prob = 1. - p

    mask = rng.binomial(x.shape, p = retain_prob, dtype = x.dtype)
    if just_dropout:
        x = x * mask / retain_prob
    else:
        x = x * mask + (1 - mask) * T.roll(x, shift = shift, axis = 0)

    return (x, retain_prob)



  class HB(Layer):
    '''Applies the hybrid bootstrap to the input.
        # Arguments
        p: float between 0 and 1. Fraction of the input units to resample if unif = F,
           maximum fraction if unif = T
        shift: int. Should be smaller than batch size.
        unif: bool.  Should p be sampled from unif(0, p)?
        just_dropout: Should we just do dropout?
        '''
    def __init__(self, p, shift, unif = True, just_dropout = False, **kwargs):
        self.init_p = p
        self.shift = shift
        self.unif = unif
        self.just_dropout = just_dropout
        self.uses_learning_phase = True
        self.supports_masking = True
        self.last_retain_prob = None
        super(HB, self).__init__(**kwargs)

    def build(self, input_shape):
        self.p = self.add_weight(shape=(),
                                 name = 'p',
                                 initializer=Constant(value = self.init_p),
                                 trainable=False)
        super(HB, self).build(input_shape)

    def call(self, x, mask=None):
        if 0. < self.p.get_value() < 1.:
            x, self.last_retain_prob = hybo(x, p = self.p, shift = self.shift,
                                        unif = self.unif,
                                        just_dropout = self.just_dropout)
        return x

    def get_config(self):
        config = {'init_p': self.init_p, 'p': self.p, 'shift': self.shift,
                  'unif': self.unif, 'just_dropout': self.just_dropout}
        base_config = super(HB, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))




  class RecordGrads(Callback):
    '''Callback to keep track of uncorrupted training losses'''
    def __init__(self, data, num_samples, sample_batch_size):
        self.data_X, self.data_Y = data
        self.num_samples = num_samples
        self.sample_batch_size = sample_batch_size

    def on_train_begin(self, logs={}):
        gradients = self.model.optimizer.get_gradients(self.model.total_loss, self.model.trainable_weights)
        inputs = [self.model.inputs[0],
                  self.model.sample_weights[0],
                  self.model.targets[0]]
        last_retain_probs = []
        for layer in self.model.layers:
            if layer.name[0:2] == 'hb':
                last_retain_probs.append(layer.last_retain_prob)
        gradients_and_ps = gradients + last_retain_probs
        self.get_gradients_and_ps = K.function(inputs = inputs, outputs = gradients_and_ps)
        self.num_lay = len(self.model.trainable_weights) / 2
        self.sampled_gradients_list = []
        self.just_first_point = np.zeros(self.sample_batch_size)
        self.just_first_point[0] = 1

    def on_epoch_end(self, batch, logs={}):
        sampled_gradients = np.zeros((self.num_samples * self.num_lay, 2))
        for i in range(self.num_samples):
            random_sample = np.random.randint(self.data_X.shape[0], size = self.sample_batch_size)
            inputs = [self.data_X[random_sample], self.just_first_point, self.data_Y[random_sample]]
            gsandps = self.get_gradients_and_ps(inputs)
            for j in range(self.num_lay):
                weight_vector = np.concatenate([np.array(gsandps[2 * j]).flatten(), np.array(gsandps[2 * j + 1]).flatten()])
                norm = np.linalg.norm(weight_vector)
                sampled_gradients[i * self.num_lay + j, 0] = norm
                sampled_gradients[i * self.num_lay + j, 1] = 1 - gsandps[2 * self.num_lay + j][0]
        self.sampled_gradients_list.append(sampled_gradients)




  class OptimizerScheduler(Callback):
    """Schedule for the optimizer.  Virtually identical to keras LearningRateScheduler but
    sets momentum (also I took out the chedule output checker)
    # Arguments
        schedule: a function that takes an epoch index as input
            (integer, indexed from 0) and returns a new
            learning rate as output (float).
    """
    def __init__(self, schedule):
        super(OptimizerScheduler, self).__init__()
        self.schedule = schedule
    def on_epoch_begin(self, epoch, logs=None):
        if not hasattr(self.model.optimizer, 'lr'):
            raise ValueError('Optimizer must have a "lr" attribute.')
        lr, momentum = self.schedule(epoch)
        K.set_value(self.model.optimizer.lr, lr)
        K.set_value(self.model.optimizer.momentum, momentum)
        

  batch_size = 128
  nb_classes = 10
  img_rows, img_cols = 28, 28
  nb_filters = 32
  nb_pool = 2
  nb_conv = 5

  (X_train, y_train), (X_test, y_test) = mnist.load_data()
  X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
  X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)
  X_train = X_train.astype('float32')
  X_test = X_test.astype('float32')
  X_train /= 255
  X_test /= 255
  Y_train = np_utils.to_categorical(y_train, nb_classes)
  Y_test = np_utils.to_categorical(y_test, nb_classes)

  small_indices = np.array([], dtype = 'int32')
  for i in range(nb_classes):
    first_100_i_indices = np.arange(X_train.shape[0])[y_train == i][0:100]
    small_indices = np.concatenate([small_indices, first_100_i_indices])

    
  small_X_train = X_train[small_indices]
  small_y_train = y_train[small_indices]
  small_Y_train = Y_train[small_indices]

  def schedule(x):
    '''Rather ugly way of defining a learning schedule function'''
    x = np.array(x, dtype = 'float32')
    lr = np.piecewise(x, [x < 500, (x >= 500) & (x < 1000), x >= 1000],
                        [0.01, 0.001, 0.0001])
    momentum = np.piecewise(x, [x < 500, x >= 500],
                        [0.9, 0.99])
    return((lr, momentum))

  frames = []
  shuffle_perm = np.arange(small_X_train.shape[0])
  np.random.shuffle(shuffle_perm)
  record_grads = RecordGrads([small_X_train[shuffle_perm], small_Y_train[shuffle_perm]], num_samples = 100, sample_batch_size = 128)
  optimizer_scheduler = OptimizerScheduler(schedule)
  just_dropout = False
  unif = True
  hbp = 0.5

  model = Sequential()
  model.add(HB(hbp / 2, shift = -1, input_shape = (img_rows, img_cols, 1), unif = unif, just_dropout = just_dropout))
  model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
  model.add(HB(hbp, shift = -2, unif = unif, just_dropout = just_dropout))
  model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
  model.add(MaxPooling2D((nb_pool, nb_pool)))
  model.add(HB(hbp, shift = -3, unif = unif, just_dropout = just_dropout))
  model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
  model.add(HB(hbp, shift = -4, unif = unif, just_dropout = just_dropout))
  model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
  model.add(MaxPooling2D((2, 2)))
  model.add(Flatten())
  model.add(HB(hbp, shift = -6, unif = unif, just_dropout = just_dropout))
  model.add(Dense(nb_classes, activation = 'softmax'))
  sgd = SGD(lr = 0.01, momentum = 0.9, decay = 0.00, nesterov = False)
  model.compile(loss='categorical_crossentropy',
              optimizer= sgd,
              metrics=['accuracy'])
  myfit = model.fit(small_X_train, small_Y_train, batch_size = batch_size, epochs = 1500,
                  verbose = 1, validation_data=(X_test, Y_test), callbacks = [record_grads, optimizer_scheduler])

  grads_and_ps = np.concatenate(record_grads.sampled_gradients_list)
  grads_and_ps = pd.DataFrame(grads_and_ps)
  grads_and_ps.to_csv("./data/grads_and_ps.csv")



#+end_src

#+RESULTS[813ee72e7d75562717e8fb0506cb96234cbf4816]: compute_grads
: None

#+name: grads_figure
#+begin_src R :exports results :file grads.pdf
  library(ggplot2)
  library(scales)
  library(gridExtra)
  data <- read.csv("./data/grads_and_ps.csv")
  colnames(data) <- c("row", "norm", "p")
  data$layer = rep(1:5, dim(data)[1] / 5)
  data$epoch = sort(rep(0:1499, dim(data)[1] / 1500))
  plots = list()
  for(layer in 1:5){
    layer_data = data[data$layer == layer,]
    plot <- ggplot(data = layer_data, aes(x = epoch, y = p, z = norm)) +
      stat_summary_hex(bins = 30) +
      xlim(c(0, 1499)) +
      ylim(c(0, 0.5))
    color_limits <- quantile(ggplot_build(plot)$data[[1]]$value, c(0.01, 0.5, 0.99))
    attributes(color_limits) <- NULL
    newplot <- plot + 
      scale_fill_gradient2(low = muted("blue"), high = muted("red"),
                           midpoint = color_limits[2],
                           limits = color_limits[c(1, 3)],
                           breaks = c(ceiling(color_limits[1]), floor(color_limits[3])),
                           name = bquote(bgroup("|", bgroup("|",frac(delta ~ "L", delta ~ "Layer" [.(layer)]),"|"),"|")[2])) + 
      geom_vline(aes(xintercept = 500), linetype = 4) + 
      geom_vline(aes(xintercept = 1000), linetype = 4) + 
      annotate("label", x = 250, y = 0.48,
               label = "lr: 0.01 \n mom: 0.9", size = 2) + 
      annotate("label", x = 750, y = 0.48,
               label = "lr: 0.001 \n mom: 0.99", size = 2) + 
      annotate("label", x = 1250, y = 0.48,
               label = "lr: 0.0001 \n mom: 0.99", size = 2)  + 
      theme(legend.text = element_text(size = 8), legend.title = element_text(size = 8)) +
      xlab("Epochs")
    # force plot evaluation
    print(newplot)
    plots[[layer]] <- newplot
  }
  plot_array <- grid.arrange(grobs = plots, nrow = 3)
  ggsave("grads.pdf", plot_array, width = 7.5,
           height = 7, units = "in")
#+end_src
#+caption: As training progresses, training points that have been corrupted less 
#+caption: have smaller gradients than more heavily corrupted points. 
#+label: gradient_figure
#+RESULTS: grads_figure
Clearly low probabilities of resampling are associated with smaller
gradients.  This relationship is somewhat less obvious for layers far
from the output because the gradient size is affected by the amount of
corruption between these layers and the output.

We have no reason to suppose that the uniform distribution is optimal
for sampling the hyperparameter $p$.  We employ it because:
1.  We can easily ensure that $p$ is between zero and one.
2.  Uniformly distributed random numbers are readily available in most software packages. 
3.  Using the uniform distribution ensures that values of $p$ near
    zero are relatively probable compared to some symmetric,
    hump-shaped alternatives.  This is a hedge to ensure regularized
    networks do not perform much worse than unregularized networks.
    For instance, using the uniform distribution helps assure that the
    optimization can ``get started,'' whereas heavily corrupted
    networks can sometimes fail to improve at all.
There are other plausible substitutes, such as the Beta distribution,
which we have not investigated.

* Structured Sampling for Convolutional Networks
  <<conv_sampling>> 

  The hybrid bootstrap of Equation [[hb_def]] does not account for the
  spatial structure exploited by glspl:cnn, so we investigated whether
  changing the sampling pattern based on this structure would improve
  the hybrid bootstrap's performance on image tasks.

  #+name:2d_hybrid_bootstrap
  #+begin_src python :exports none
    import numpy as np
    from keras import backend as K
    from theano import tensor as T
    from keras.engine.topology import Layer
    from keras.initializers import Constant

    def hybo_2d_conv(x, p, shift, seed = None, unif = True, just_dropout = False):
        '''Theano hybrid bootstrap backend'''
        if p.get_value() < 0. or p.get_value() > 1:
            raise Exception('Hybrid bootstrap p must be in interval [0, 1].')

        if seed is None:
            seed = np.random.randint(1, 10e6)
            rng = K.RandomStreams(seed = seed)

        if(unif == True):
            retain_prob = 1. - rng.uniform((x.shape[0],), 0, p, dtype = x.dtype)
            for dim in range(x.ndim - 1):
                retain_prob = K.expand_dims(retain_prob, dim + 1)
        else:
            retain_prob = 1. - p
            
        mask = rng.binomial((x.shape[0], x.shape[1], x.shape[2], 1), p=retain_prob, dtype=x.dtype)
        mask = T.extra_ops.repeat(mask, x.shape[3], axis = 3)

        if just_dropout:
            x = x * mask / retain_prob
        else:
            x = x * mask + (1 - mask) * T.roll(x, shift = shift, axis = 0)

        return x



    class HB_2d_conv(Layer):
        '''Applies the hybrid bootstrap to the input.
            # Arguments
            p: float between 0 and 1. Fraction of the input units to resample if unif = F,
               maximum fraction if unif = T
            shift: int. Should be smaller than batch size.
            unif: bool.  Should p be sampled from unif(0, p)?
            just_dropout: Should we just do dropout?
            '''
        def __init__(self, p, shift, unif = True, just_dropout = False, **kwargs):
            self.init_p = p
            self.shift = shift
            self.unif = unif
            self.just_dropout = just_dropout
            self.uses_learning_phase = True
            self.supports_masking = True
            super(HB_2d_conv, self).__init__(**kwargs)

        def build(self, input_shape):
            self.p = self.add_weight(shape=(),
                                     name = 'p',
                                     initializer=Constant(value = self.init_p),
                                     trainable=False)
            super(HB_2d_conv, self).build(input_shape)

        def call(self, x, mask=None):
            if 0. < self.p.get_value() < 1.:
                x = K.in_train_phase(hybo_2d_conv(x, p = self.p, shift = self.shift,
                                          unif = self.unif,
                                          just_dropout = self.just_dropout), x)
            return x

        def get_config(self):
            config = {'init_p': self.init_p, 'p': self.p, 'shift': self.shift,
                      'unif': self.unif, 'just_dropout': self.just_dropout}
            base_config = super(HB_2d_conv, self).get_config()
            return dict(list(base_config.items()) + list(config.items()))

  #+end_src

  #+RESULTS: 2d_hybrid_bootstrap
  : None

  #+name:channel_dropout
  #+begin_src python :exports none
    import numpy as np
    from keras import backend as K
    from theano import tensor as T
    from keras.engine.topology import Layer
    from keras.initializers import Constant

    def hybo_channel(x, p, shift, seed = None, unif = True, just_dropout = False):
        '''Theano hybrid bootstrap backend'''
        if p.get_value() < 0. or p.get_value() > 1:
            raise Exception('Hybrid bootstrap p must be in interval [0, 1].')

        if seed is None:
            seed = np.random.randint(1, 10e6)
            rng = K.RandomStreams(seed = seed)

        if(unif == True):
            retain_prob = 1. - rng.uniform((x.shape[0],), 0, p, dtype = x.dtype)
            for dim in range(x.ndim - 1):
                retain_prob = K.expand_dims(retain_prob, dim + 1)
        else:
            retain_prob = 1. - p

        mask = rng.binomial((x.shape[0], 1, 1, x.shape[3]), p=retain_prob, dtype=x.dtype)
        mask = T.extra_ops.repeat(mask, x.shape[1], axis = 1)
        mask = T.extra_ops.repeat(mask, x.shape[2], axis = 2)

        if just_dropout:
            x = x * mask / retain_prob
        else:
            x = x * mask + (1 - mask) * T.roll(x, shift = shift, axis = 0)
        return x



    class HB_channel(Layer):
        '''Applies the hybrid bootstrap to the input.
            # Arguments
            p: float between 0 and 1. Fraction of the input units to resample if unif = F,
               maximum fraction if unif = T
            shift: int. Should be smaller than batch size.
            unif: bool.  Should p be sampled from unif(0, p)?
            just_dropout: Should we just do dropout?
            '''
        def __init__(self, p, shift, unif = True, just_dropout = False, **kwargs):
            self.init_p = p
            self.shift = shift
            self.unif = unif
            self.just_dropout = just_dropout
            self.uses_learning_phase = True
            self.supports_masking = True
            super(HB_channel, self).__init__(**kwargs)
        def build(self, input_shape):
            self.p = self.add_weight(shape=(),
                                     name = 'p',
                                     initializer=Constant(value = self.init_p),
                                     trainable=False)
            super(HB_channel, self).build(input_shape)
        def call(self, x, mask=None):
            if 0. < self.p.get_value() < 1.:
                x = K.in_train_phase(hybo_channel(x, p = self.p, shift = self.shift,
                                          unif = self.unif,
                                          just_dropout = self.just_dropout), x)
            return x
        def get_config(self):
            config = {'init_p': self.init_p, 'p': self.p, 'shift': self.shift,
                      'unif': self.unif, 'just_dropout': self.just_dropout}
            base_config = super(HB_channel, self).get_config()
            return dict(list(base_config.items()) + list(config.items()))

  #+end_src

  #+RESULTS: channel_dropout
  : None

  
  #+name:sampling_visualization_data_generator
  #+begin_src python :exports none :cache yes :noweb yes
    import numpy as np
    np.random.seed(42)
    import pandas as pd
    from keras.datasets import mnist
    from keras.models import Sequential, Model
    from keras.layers import Dense, Dropout, Activation, Flatten, Input
    from keras.layers.convolutional import Conv2D
    from keras.layers.pooling import MaxPooling2D
    from keras.utils import np_utils
    from keras.optimizers import SGD
    from sklearn.metrics import log_loss
    from keras.callbacks import Callback

    <<basic_hb>>

    class OptimizerScheduler(Callback):
        """Schedule for the optimizer.  Virtually identical to keras LearningRateScheduler but
        sets momentum (also I took out the chedule output checker)
        # Arguments
            schedule: a function that takes an epoch index as input
                (integer, indexed from 0) and returns a new
                learning rate as output (float).
        """
        def __init__(self, schedule):
            super(OptimizerScheduler, self).__init__()
            self.schedule = schedule
        def on_epoch_begin(self, epoch, logs=None):
            if not hasattr(self.model.optimizer, 'lr'):
                raise ValueError('Optimizer must have a "lr" attribute.')
            lr, momentum = self.schedule(epoch)
            K.set_value(self.model.optimizer.lr, lr)
            K.set_value(self.model.optimizer.momentum, momentum)


    batch_size = 128
    nb_classes = 10
    img_rows, img_cols = 28, 28
    nb_filters = 5
    nb_pool = 2
    nb_conv = 5

    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255
    Y_train = np_utils.to_categorical(y_train, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)

    small_indices = np.array([], dtype = 'int32')
    for i in range(nb_classes):
        first_100_i_indices = np.arange(X_train.shape[0])[y_train == i][0:100]
        small_indices = np.concatenate([small_indices, first_100_i_indices])

        
    small_X_train = X_train[small_indices]
    small_y_train = y_train[small_indices]
    small_Y_train = Y_train[small_indices]

    def schedule(x):
        '''Rather ugly way of defining a learning schedule function'''
        x = np.array(x, dtype = 'float32')
        lr = np.piecewise(x, [x < 500, (x >= 500) & (x < 1000), (x >= 1000) & (x < 1500), x >= 1500],
                            [0.01, 0.001, 0.0001, 0.00001])
        momentum = np.piecewise(x, [x < 500, x >= 500],
                            [0.9, 0.99])
        return((lr, momentum))

    optimizer_scheduler = OptimizerScheduler(schedule)
    just_dropout = False
    unif = True
    sgd = SGD(lr = 0.01, momentum = 0.9, decay = 0.00, nesterov = False)
    hbp = 0.45
    model = Sequential()
    model.add(HB(hbp / 2, shift = -1, input_shape = (img_rows, img_cols, 1), unif = unif, just_dropout = just_dropout))
    model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
    model.add(HB(hbp, shift = -2, unif = unif, just_dropout = just_dropout))
    model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
    model.add(MaxPooling2D((nb_pool, nb_pool)))
    model.add(HB(hbp, shift = -3, unif = unif, just_dropout = just_dropout))
    model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
    model.add(HB(hbp, shift = -4, unif = unif, just_dropout = just_dropout))
    model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(HB(hbp, shift = -6, unif = unif, just_dropout = just_dropout))
    model.add(Dense(nb_classes, activation = 'softmax'))
    model.compile(loss ='categorical_crossentropy',
                  optimizer = sgd,
                  metrics = ['accuracy'])
    myfit = model.fit(small_X_train, small_Y_train, batch_size = batch_size, epochs = 2000,
                          verbose = 0, callbacks = [optimizer_scheduler])


    preds = model.predict(X_test)
    preds = np.argmax(preds, axis = 1)
    np.sum(preds == y_test)

    for i in range(11):
        model.pop()

    model.compile(loss ='mse',
                  optimizer = sgd)

    image_maps_0 = model.predict(X_train)[0]
    image_maps_0 = [image_maps_0[ :, :, channel].flatten() for channel in range(nb_filters)]
    df_0 = pd.DataFrame(image_maps_0).T
    image_maps_1 = model.predict(X_train)[1]
    image_maps_1 = [image_maps_1[ :, :, channel].flatten() for channel in range(nb_filters)]
    df_1 = pd.DataFrame(image_maps_1).T
    df_0.to_csv('./data/image_maps_0')
    df_1.to_csv('./data/image_maps_1')
  #+end_src

  #+RESULTS[3dbfc3c01fd95df7365ae1d4d2c7f6497045f22a]: sampling_visualization_data_generator
  : None

  #+name:sampling_visualization
  #+begin_src R :exports results :file conv_sampling.pdf
    set.seed(42)
    library(png)
    library(grid)
    library(ggplot2)

    scale_factor <- 0.9

    hbp = 0.5
    image_maps_0 <- read.csv("./data/image_maps_0")[, -1]
    image_maps_1 <- read.csv("./data/image_maps_1")[, -1]
    max_pixel <- max(rbind(image_maps_0, image_maps_1))
    image_maps_0 <- lapply(image_maps_0, function(x){matrix(x, 24, 24) / max_pixel})
    image_maps_1 <- lapply(image_maps_1, function(x){matrix(x, 24, 24) / max_pixel})

    num_maps = length(image_maps_1)

    hybrid_bootstrap <- function(x, p){
      x_dim <- dim(x[[1]])
      mask <- matrix(rbinom(x_dim[1] * x_dim[2], size = 1, prob = 1 - p), x_dim[1], x_dim[2])
      sample <- x[[1]] * mask + x[[2]] * (1 - mask)
      return(t(sample))}

    hybrid_bootstrap_2d_conv <- function(x, p){
      set.seed(42)
      x_dim <- dim(x[[1]])
      mask <- matrix(rbinom(x_dim[1] * x_dim[2], size = 1, prob = 1 - p), x_dim[1], x_dim[2])
      sample <- x[[1]] * mask + x[[2]] * (1 - mask)
      return(t(sample))}

    channel_bootstrap <- function(x, p){
      x_dim <- dim(x[[1]])
      mask <- rbinom(1, size = 1, prob = 1 - p)
      sample <- x[[1]] * mask + x[[2]] * (1 - mask)
      return(t(sample))}

    maps <- 1:num_maps
    hack_maps <- seq(1 - 0.5, num_maps + 0.5, length.out = 4)
    df <- data.frame(Method = factor(c("Original Map",
                                       "Basic Hybrid Bootstrap",
                                       "Spatial Grid Hybrid Bootstrap for CNN",
                                       "Channel Hybrid Bootstrap"),
                                     levels = rev(c("Original Map",
                                                "Basic Hybrid Bootstrap",
                                                "Spatial Grid Hybrid Bootstrap for CNN",
                                                "Channel Hybrid Bootstrap"))),
                     maps = hack_maps)

    plot <- ggplot(df, aes(maps, Method)) +
      scale_x_continuous(breaks = 1:5) + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
      geom_blank() +
      xlab("Maps")


    for(i in 1:num_maps){
      image <- t(image_maps_0[[i]])
      image <- image / max(image)
      g <- rasterGrob(image)
      plot <- plot + annotation_custom(g, xmin = maps[i] - 0.5 * scale_factor, xmax = maps[i] + 0.5 * scale_factor,
                                       ymin = 4 - 0.5 * scale_factor, ymax = 4 + 0.5 * scale_factor)
    }

    for(i in 1:num_maps){
      image <- hybrid_bootstrap(list(image_maps_0[[i]], image_maps_1[[i]]), hbp)
      image <- image / max(image)
      g <- rasterGrob(image)
      plot <- plot + annotation_custom(g, xmin = maps[i] - 0.5 * scale_factor, xmax = maps[i] + 0.5 * scale_factor,
                                       ymin = 3 - 0.5 * scale_factor, ymax = 3 + 0.5 * scale_factor)
    }

    for(i in 1:num_maps){
      image <- hybrid_bootstrap_2d_conv(list(image_maps_0[[i]], image_maps_1[[i]]), hbp)
      image <- image / max(image)
      g <- rasterGrob(image)
      plot <- plot + annotation_custom(g, xmin = maps[i] - 0.5 * scale_factor, xmax = maps[i] + 0.5 * scale_factor,
                                       ymin = 2 - 0.5 * scale_factor, ymax = 2 + 0.5 * scale_factor)
    }

    for(i in 1:num_maps){
      image <- channel_bootstrap(list(image_maps_0[[i]], image_maps_1[[i]]), hbp)
      image <- image / max(image)
      g <- rasterGrob(image)
      plot <- plot + annotation_custom(g, xmin = maps[i] - 0.5 * scale_factor, xmax = maps[i] + 0.5 * scale_factor,
                                       ymin = 1 - 0.5 * scale_factor, ymax = 1 + 0.5 * scale_factor)
    }

    ggsave("conv_sampling.pdf", plot, device = "pdf", width = 6.5,
           height = 4, units = "in")

  #+end_src
  #+caption: Visualization of different hybrid bootstrap sampling schemes for glspl:cnn.
  #+label: sampling_visualization
  #+attr_latex: :width 0.9\linewidth
  #+RESULTS: sampling_visualization
  [[file:conv_sampling.pdf]]

  In particular, we wondered if glspl:cnn would develop redundant
  filters to ``solve'' the problem of the hybrid bootstrap since the
  resampling locations are chosen independently for each filter.  We
  therefore considered using the same spatial swapping pattern for
  every filter, which we call the spatial grid hybrid bootstrap since
  pixel positions are either swapped or not.  Tompson et. al
  considered dropping whole filters as a modified form of dropout that
  they call SpatialDropout (their justification is also spatial)
  cite:tompson2015efficient.  This approach seems a little extreme in
  the case of the hybrid bootstrap because the whole feature map would
  be swapped, but perhaps it could work since the majority of feature
  maps will still be associated with the target class.  We call this
  variant the channel hybrid bootstrap to avoid confusion with the
  spatial grid hybrid bootstrap.
  
  The feature maps following regularization corresponding to these
  schemes are visualized in Figure [[sampling_visualization]]. It is
  difficult to visually distinguish the spatial grid hybrid bootstrap from the
  basic hybrid bootstrap even though the feature maps for the spatial grid
  hybrid bootstrap are all swapped at the same locations, whereas the
  locations for the basic hybrid bootstrap are independently chosen.
  This may explain their similar performance.

  We compare the error rates of the three hybrid bootstrap schemes in
  the top left panel of Figure [[sampling_correlation_figure]] for various
  values of $u$ by training on 1,000 points and computing the accuracy
  on the remaining 59,000 points in the conventional training set. Both
  the spatial grid and the channel hybrid bootstrap outperform the
  basic hybrid bootstrap for low levels of corruption.  As $u$
  increases, the basic hybrid bootstrap and the spatial grid hybrid
  bootstrap reach similar levels of performance.  Both methods reach a
  (satisfyingly flat) peak at approximately $u = 0.45$.  As indicated
  in the top right panel of Figure [[sampling_correlation_figure]], the
  test accuracies of both the basic hybrid bootstrap and the spatial
  grid hybrid bootstrap are similar for different initializations of
  the network at this chosen parameter. We compare the redundancy of
  networks regularized using the three hybrid bootstrap variants at
  this level of corruption.

  #+name:sampling_validation
  #+begin_src python :exports none :cache yes :noweb yes
    import numpy as np
    np.random.seed(42)
    import pandas as pd
    from keras.datasets import mnist
    from keras.models import Sequential, Model
    from keras.layers import Dense, Dropout, Activation, Flatten, Input
    from keras.layers.convolutional import Conv2D
    from keras.layers.pooling import MaxPooling2D
    from keras.utils import np_utils
    from keras.optimizers import SGD
    from sklearn.metrics import log_loss
    from keras.callbacks import Callback
    from scipy.linalg import fractional_matrix_power

    <<basic_hb>>
    <<2d_hybrid_bootstrap>>
    <<channel_dropout>>

    class OptimizerScheduler(Callback):
        """Schedule for the optimizer.  Virtually identical to keras LearningRateScheduler but
        sets momentum (also I took out the chedule output checker)
        # Arguments
            schedule: a function that takes an epoch index as input
                (integer, indexed from 0) and returns a new
                learning rate as output (float).
        """
        def __init__(self, schedule):
            super(OptimizerScheduler, self).__init__()
            self.schedule = schedule
        def on_epoch_begin(self, epoch, logs=None):
            if not hasattr(self.model.optimizer, 'lr'):
                raise ValueError('Optimizer must have a "lr" attribute.')
            lr, momentum = self.schedule(epoch)
            K.set_value(self.model.optimizer.lr, lr)
            K.set_value(self.model.optimizer.momentum, momentum)


    batch_size = 128
    nb_classes = 10
    img_rows, img_cols = 28, 28
    nb_filters = 32
    nb_pool = 2
    nb_conv = 5

    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255
    Y_train = np_utils.to_categorical(y_train, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)

    small_indices = np.array([], dtype = 'int32')
    for i in range(nb_classes):
        first_100_i_indices = np.arange(X_train.shape[0])[y_train == i][0:100]
        small_indices = np.concatenate([small_indices, first_100_i_indices])

        
    small_X_train = X_train[small_indices]
    small_y_train = y_train[small_indices]
    small_Y_train = Y_train[small_indices]

    complement_indices = np.setdiff1d(np.arange(X_train.shape[0]), small_indices)
    small_X_train_complement = X_train[complement_indices]
    small_y_train_complement = y_train[complement_indices]
    small_Y_train_complement = Y_train[complement_indices]

    def schedule(x):
        '''Rather ugly way of defining a learning schedule function'''
        x = np.array(x, dtype = 'float32')
        lr = np.piecewise(x, [x < 500, (x >= 500) & (x < 1000), (x >= 1000) & (x < 1500), x >= 1500],
                            [0.01, 0.001, 0.0001, 0.00001])
        momentum = np.piecewise(x, [x < 500, x >= 500],
                            [0.9, 0.99])
        return((lr, momentum))

    frames = []
    optimizer_scheduler = OptimizerScheduler(schedule)
    just_dropout = False
    unif = True
    sgd = SGD(lr = 0.01, momentum = 0.9, decay = 0.00, nesterov = False)

    def compute_accuracy(hbp, bootstrap_function):
        model = Sequential()
        model.add(bootstrap_function(hbp / 2, shift = -1, input_shape = (img_rows, img_cols, 1), unif = unif, just_dropout = just_dropout))
        model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
        model.add(bootstrap_function(hbp, shift = -2, unif = unif, just_dropout = just_dropout))
        model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
        model.add(MaxPooling2D((nb_pool, nb_pool)))
        model.add(bootstrap_function(hbp, shift = -3, unif = unif, just_dropout = just_dropout))
        model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
        model.add(bootstrap_function(hbp, shift = -4, unif = unif, just_dropout = just_dropout))
        model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
        model.add(MaxPooling2D((2, 2)))
        model.add(Flatten())
        model.add(HB(hbp, shift = -6, unif = unif, just_dropout = just_dropout))
        model.add(Dense(nb_classes, activation = 'softmax'))
        model.compile(loss ='categorical_crossentropy',
                      optimizer = sgd,
                      metrics = ['accuracy'])
        myfit = model.fit(small_X_train, small_Y_train, batch_size = batch_size, epochs = 2000,
                              verbose = 0, callbacks = [optimizer_scheduler])
        preds = np.argmax(model.predict(small_X_train_complement), axis = 1)
        accuracy = 1. * np.sum(preds == small_y_train_complement) / preds.shape[0]
        return([accuracy, hbp, bootstrap_function.__name__])

    funcs = [HB, HB_2d_conv, HB_channel]
    hbps = np.linspace(0, 0.5, 11)



    accuracies = []
    for func in funcs:
        for hbp in hbps:
            output = compute_accuracy(hbp, func)
            accuracies.append(output)


    df = pd.DataFrame(accuracies)
    df.to_csv('./data/sampling_accuracy_validation.csv')
  #+end_src

  #+RESULTS[11f1252b5adc993237f6d8cb8120156192cfec1f]: sampling_validation
  : None

  #+name:build_basic_and_2d_hybrid_bootstrap_networks
  #+begin_src python :exports none :cache yes :noweb yes
    import numpy as np
    np.random.seed(42)
    import pandas as pd
    from keras.datasets import mnist
    from keras.models import Sequential, Model
    from keras.layers import Dense, Dropout, Activation, Flatten, Input
    from keras.layers.convolutional import Conv2D
    from keras.layers.pooling import MaxPooling2D
    from keras.utils import np_utils
    from keras.optimizers import SGD
    from sklearn.metrics import log_loss
    from keras.callbacks import Callback
    from scipy.linalg import fractional_matrix_power

    <<basic_hb>>
    <<2d_hybrid_bootstrap>>
    <<channel_dropout>>

    class OptimizerScheduler(Callback):
        """Schedule for the optimizer.  Virtually identical to keras LearningRateScheduler but
        sets momentum (also I took out the chedule output checker)
        # Arguments
            schedule: a function that takes an epoch index as input
                (integer, indexed from 0) and returns a new
                learning rate as output (float).
        """
        def __init__(self, schedule):
            super(OptimizerScheduler, self).__init__()
            self.schedule = schedule
        def on_epoch_begin(self, epoch, logs=None):
            if not hasattr(self.model.optimizer, 'lr'):
                raise ValueError('Optimizer must have a "lr" attribute.')
            lr, momentum = self.schedule(epoch)
            K.set_value(self.model.optimizer.lr, lr)
            K.set_value(self.model.optimizer.momentum, momentum)

    def get_channels_output(model):
        channels_output_list = []
        for j in range(len(model.layers)):
            if not 'conv2d' in model.layers[-1].name:
                model.pop()
            else:
                model.compile(loss = 'mse',
                              optimizer = sgd)
                preds = model.predict(X_test)
                channels_output = [preds[:, :, :, channel].flatten() for channel in range(nb_filters)]
                channels_output = np.array(channels_output).T
                channels_output_list.append(channels_output)
                model.pop()
        return(channels_output_list)

    def sphere_neurons(model):
        for layer in range(len(model.layers)):
            if 'conv2d' in model.layers[layer].name:
                modelx = Sequential()
                modelx.add(HB(hbp / 2, shift = -1, input_shape = (img_rows, img_cols, 1), unif = unif, just_dropout = just_dropout))
                modelx.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
                modelx.add(HB(hbp, shift = -2, unif = unif, just_dropout = just_dropout))
                modelx.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
                modelx.add(MaxPooling2D((nb_pool, nb_pool)))
                modelx.add(HB(hbp, shift = -3, unif = unif, just_dropout = just_dropout))
                modelx.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
                modelx.add(HB(hbp, shift = -4, unif = unif, just_dropout = just_dropout))
                modelx.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
                modelx.add(MaxPooling2D((2, 2)))
                modelx.add(Flatten())
                modelx.add(HB(hbp, shift = -6, unif = unif, just_dropout = just_dropout))
                modelx.add(Dense(nb_classes, activation = 'softmax'))
                sgd = SGD(lr = 0.01, momentum = 0.9, decay = 0.00, nesterov = False)
                for i in range((len(model.layers) - 1) - layer):
                    modelx.pop()
                modelx.compile(loss ='categorical_crossentropy',
                              optimizer = sgd,
                              metrics = ['accuracy'])
                modelx.set_weights(model.get_weights())
                preds = modelx.predict(small_X_train)
                channels_output = [preds[:, :, :, channel].flatten() for channel in range(nb_filters)]
                channels_output = np.array(channels_output).T
                channels_cov = np.cov(channels_output, rowvar = False)
                sphere_channels = fractional_matrix_power(channels_cov, -1 / 2.)
                current_weights = model.layers[layer].get_weights()
                for i in range(nb_filters):
                    new_weight = sphere_channels[i, 0] * modelx.layers[layer].get_weights()[0][:, :, :, 0]
                    for j in range(1, nb_filters):
                        new_weight += sphere_channels[i, j] * model.layers[layer].get_weights()[0][:, :, :, j]
                    current_weights[0][:, :, :, i] = new_weight
                model.layers[layer].set_weights(current_weights)
                print(layer)
                


            

    batch_size = 128
    nb_classes = 10
    img_rows, img_cols = 28, 28
    nb_filters = 32
    nb_pool = 2
    nb_conv = 5

    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255
    Y_train = np_utils.to_categorical(y_train, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)

    small_indices = np.array([], dtype = 'int32')
    for i in range(nb_classes):
        first_100_i_indices = np.arange(X_train.shape[0])[y_train == i][0:100]
        small_indices = np.concatenate([small_indices, first_100_i_indices])

        
    small_X_train = X_train[small_indices]
    small_y_train = y_train[small_indices]
    small_Y_train = Y_train[small_indices]

    def schedule(x):
        '''Rather ugly way of defining a learning schedule function'''
        x = np.array(x, dtype = 'float32')
        lr = np.piecewise(x, [x < 500, (x >= 500) & (x < 1000), (x >= 1000) & (x < 1500), x >= 1500],
                            [0.01, 0.001, 0.0001, 0.00001])
        momentum = np.piecewise(x, [x < 500, x >= 500],
                            [0.9, 0.99])
        return((lr, momentum))

    frames = []
    optimizer_scheduler = OptimizerScheduler(schedule)
    just_dropout = False
    unif = True
    hbp = 0.45
    sgd = SGD(lr = 0.01, momentum = 0.9, decay = 0.00, nesterov = False)

    def compute_model_correlations_and_accuracy(num_replicates, sphere, bootstrap_function):
        correlations = []
        accuracies = []
        dead_neurons = []
        for replicate in range(num_replicates):
            model = Sequential()
            model.add(bootstrap_function(hbp / 2, shift = -1, input_shape = (img_rows, img_cols, 1), unif = unif, just_dropout = just_dropout))
            model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
            model.add(bootstrap_function(hbp, shift = -2, unif = unif, just_dropout = just_dropout))
            model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
            model.add(MaxPooling2D((nb_pool, nb_pool)))
            model.add(bootstrap_function(hbp, shift = -3, unif = unif, just_dropout = just_dropout))
            model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
            model.add(bootstrap_function(hbp, shift = -4, unif = unif, just_dropout = just_dropout))
            model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
            model.add(MaxPooling2D((2, 2)))
            model.add(Flatten())
            model.add(HB(hbp, shift = -6, unif = unif, just_dropout = just_dropout))
            model.add(Dense(nb_classes, activation = 'softmax'))
            model.compile(loss ='categorical_crossentropy',
                          optimizer = sgd,
                          metrics = ['accuracy'])
            if(sphere):
                sphere_neurons(model)
                last_weights = model.layers[12].get_weights()
                last_weights[0] = last_weights[0] / 10.
                model.layers[12].set_weights(last_weights)
            saved_weights = model.get_weights()
            channels_output = get_channels_output(model)
            for layer in range(4):
                mean_abs_correlation = np.nanmean(np.abs(np.corrcoef(channels_output[layer], rowvar = False)))
                median_abs_correlation = np.nanmedian(np.abs(np.corrcoef(channels_output[layer], rowvar = False)))
                correlations.append(['initial', replicate, layer, mean_abs_correlation, median_abs_correlation, sphere, bootstrap_function.__name__])
                dead_neurons.append(['initial', replicate, np.sum(np.max(channels_output[layer], axis = 0) == 0), sphere, bootstrap_function.__name__])
            model = Sequential()
            model.add(bootstrap_function(hbp / 2, shift = -1, input_shape = (img_rows, img_cols, 1), unif = unif, just_dropout = just_dropout))
            model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
            model.add(bootstrap_function(hbp, shift = -2, unif = unif, just_dropout = just_dropout))
            model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
            model.add(MaxPooling2D((nb_pool, nb_pool)))
            model.add(bootstrap_function(hbp, shift = -3, unif = unif, just_dropout = just_dropout))
            model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
            model.add(bootstrap_function(hbp, shift = -4, unif = unif, just_dropout = just_dropout))
            model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
            model.add(MaxPooling2D((2, 2)))
            model.add(Flatten())
            model.add(HB(hbp, shift = -6, unif = unif, just_dropout = just_dropout))
            model.add(Dense(nb_classes, activation = 'softmax'))
            model.compile(loss ='categorical_crossentropy',
                          optimizer = sgd,
                          metrics = ['accuracy'])
            model.set_weights(saved_weights)
            myfit = model.fit(small_X_train, small_Y_train, batch_size = batch_size, epochs = 2000,
                              verbose = 0, callbacks = [optimizer_scheduler])
            preds = np.argmax(model.predict(X_test), axis = 1)
            accuracy = 1. * np.sum(preds == y_test) / preds.shape[0]
            accuracies.append([replicate, accuracy, sphere, bootstrap_function.__name__])
            channels_output = get_channels_output(model)
            for layer in range(4):
                mean_abs_correlation = np.nanmean(np.abs(np.corrcoef(channels_output[layer], rowvar = False)))
                median_abs_correlation = np.nanmedian(np.abs(np.corrcoef(channels_output[layer], rowvar = False)))
                correlations.append(['after training', replicate, layer, mean_abs_correlation, median_abs_correlation, sphere, bootstrap_function.__name__])
                dead_neurons.append(['after training', replicate, np.sum(np.max(channels_output[layer], axis = 0) == 0), sphere, bootstrap_function.__name__])
        return((correlations, accuracies, dead_neurons))

    nb_replicates = 10
    funcs = [HB, HB_2d_conv, HB_channel]
    spheres = [True, False]

    correlations = []
    accuracies = []
    dead_neurons = []
    for func in funcs:
        for sphere in spheres:
            output = compute_model_correlations_and_accuracy(nb_replicates, sphere, func)
            correlations.append(output[0])
            accuracies.append(output[1])
            dead_neurons.append(output[2])


    correlations = pd.DataFrame(np.concatenate(correlations))
    accuracies = pd.DataFrame(np.concatenate(accuracies))
    dead_neurons = pd.DataFrame(np.concatenate(dead_neurons))
    correlations.to_csv('./data/sampling_correlations.csv')
    accuracies.to_csv('./data/sampling_accuracies.csv')
    dead_neurons.to_csv('./data/sampling_dead_neurons.csv')
#+end_src

  #+RESULTS[fb3d18c85f75bd548227c34e8e623f01ab52f6a8]: build_basic_and_2d_hybrid_bootstrap_networks
  : None

  One possible measure of the redundancy of filters in
  a particular layer of a gls:cnn is the average absolute correlation
  between the output of the filters.  We consider the median absolute
  correlation for 10 different initializations in the bottom panel of
  Figure [[sampling_correlation_figure]].  The middle two layers exhibit
  the pattern we expected: the spatial grid hybrid bootstrap leads to
  relatively small correlations between filters.  However, this
  pattern does not hold for the first and last convolutional layer.
  If we attempt to reduce the initial absolute correlations of the
  filters with a rotation, even this pattern does not hold up.

  #+name:structured_sampling_figure
  #+begin_src R :exports results :file spatial_plots.pdf
    library(ggplot2)
    library(grid)
    library(gridExtra)


    accuracies <- read.csv("./data/sampling_accuracy_validation.csv")
    accuracies <- accuracies[, -1]
    colnames(accuracies) <- c("accuracy", "hbp", "method")
    accuracies$method <- factor(accuracies$method, levels = c("HB", "HB_2d_conv", "HB_channel"),
                                labels = c("Basic\nHybrid\nBootstrap\n",
                                           "Spatial\nGrid\nHybrid\nBootstrap\n",
                                           "Channel\nHybrid\nBootstrap\n"))

    validation_plot <- qplot(data = accuracies, x = hbp, y = 1 - accuracy, color = method, geom = c("point", "line")) +
      xlab(expression(italic(u))) +
      ylab("Misclassification Rate") +
      scale_color_discrete(name = "Method")

    accuracies <- read.csv("./data/sampling_accuracies.csv")
    accuracies <- accuracies[, -1]
    colnames(accuracies) <- c("replicate", "accuracy", "sphere", "method")
    accuracies$method <- factor(accuracies$method, levels = c("HB", "HB_2d_conv", "HB_channel"),
                                labels = c("Basic\nHybrid\nBootstrap\n",
                                           "Spatial\nGrid\nHybrid\nBootstrap\n",
                                           "Channel\nHybrid\nBootstrap\n")) 
    accuracies$sphere <- factor(accuracies$sphere, levels = c("False", "True"),
                                labels = c("Usual", "Pre-sphered"))
    test_plot <- ggplot(data = accuracies[accuracies$sphere != "Pre-sphered",], aes(x = method, y = 1 - accuracy, group = method)) + 
      geom_boxplot(aes(fill = method)) +
      theme(axis.title.x = element_blank(),
            axis.text.x = element_blank(),
            axis.ticks.x = element_blank()) +
      ylab("Misclassification Rate") +
      scale_fill_discrete(name = "Method")

    correlations <- read.csv("./data/sampling_correlations.csv")
    correlations <- correlations[, -1]
    colnames(correlations) <- c("phase", "replicate", "layer", "mean correlation", "median correlation", "sphere", "method")
    correlations$method <- factor(correlations$method, levels = c("HB", "HB_2d_conv", "HB_channel"),
                                  labels = c("Basic\nHybrid\nBootstrap\n",
                                             "Spatial\nGrid\nHybrid\nBootstrap\n",
                                             "Channel\nHybrid\nBootstrap\n")) 

    correlations$sphere <- factor(correlations$sphere, levels = c("False", "True"),
                                  labels = c("Usual", "Pre-sphered"))
    correlations$layer <- 4 - correlations$layer
    correlations$layer <- factor(correlations$layer, levels = 1:4,
                                 labels = paste("Layer", 1:4, sep = " "))

    correlations <- correlations[correlations$phase == "after training", ]
    correlation_plot <- ggplot(data = correlations[correlations$sphere != "Pre-sphered",], aes(x = method, y = `median correlation`, group = method)) + 
      geom_boxplot(aes(fill = method)) +
      facet_grid(. ~ layer) +
      theme(axis.title.x = element_blank(),
            axis.text.x = element_blank(),
            axis.ticks.x = element_blank()) +
      ylab("Median Absolute Activation Correlation") +
      scale_fill_discrete(name = "Method") +
      ylim(0, 0.3)

    layout <- matrix(c(1,3,2,3), 2, 2)
    plot <- grid.arrange(grobs = list(validation_plot, test_plot, correlation_plot), layout_matrix = layout)

    ggsave("spatial_plots.pdf",
               plot,
               device = "pdf",
               width = 8.5,
               height = 6.5,
               units = "in")
  #+end_src
  #+caption: Validation accuracy of structured sampling schemes using 1,000
  #+caption:  training images (top left), test accuracy of structured sampling
  #+caption: schemes for 10 different initializations (top right), and  
  #+caption: median absolute correlations of neurons in convolutional 
  #+caption: layers following training for 10 initializations (bottom).
  #+label: sampling_correlation_figure
  #+RESULTS: structured_sampling_figure
  [[file:spatial_plots.pdf]]


  Overall, the difference in performance between the spatial grid
  hybrid bootstrap and the basic hybrid bootstrap is modest,
  particularly near their optimal parameter value.  We use the spatial
  grid hybrid bootstrap for glspl:cnn on the basis that it seems to
  perform at least as well as the basic hybrid bootstrap, and
  outperforms the basic hybrid bootstrap if we select a $u$ that is
  too small.
* Performance as a Function of Number of Training Examples
  <<perf_vs_size>>

  #+name: accuracy_vs_n_data_generator
  #+begin_src python :exports none :noweb yes :cache yes
    import numpy as np
    np.random.seed(42)
    import pandas as pd
    from keras.datasets import mnist
    from keras.models import Sequential
    from keras.layers import Dense, Dropout, Activation, Flatten
    from keras.layers.convolutional import Conv2D
    from keras.layers.pooling import MaxPooling2D
    from keras.utils import np_utils
    from keras.optimizers import SGD
    from sklearn.metrics import log_loss
    from keras.callbacks import Callback

    <<basic_hb>>
    <<2d_hybrid_bootstrap>>

    class UncorruptedTrainHistory(Callback):
        '''Callback to keep track of uncorrupted training losses'''
        def __init__(self, data):
            self.data_X, self.data_Y = data
        
        def on_train_begin(self, logs={}):
            self.losses = []
            self.accs = []
        
        def on_epoch_end(self, batch, logs={}):
            loss, acc = self.model.evaluate(self.data_X, self.data_Y)
            print("\nUncorrupted Training Loss: {}, Uncorrupted Training Accuracy: {}" .format(loss, acc))
            self.losses.append(loss)
            self.accs.append(acc)
            
    class OptimizerScheduler(Callback):
        """Schedule for the optimizer.  Virtually identical to keras LearningRateScheduler but
        sets momentum (also I took out the schedule output checker)
        # Arguments
            schedule: a function that takes an epoch index as input
                (integer, indexed from 0) and returns a new
                learning rate as output (float).
        """
        def __init__(self, schedule):
            super(OptimizerScheduler, self).__init__()
            self.schedule = schedule
        def on_epoch_begin(self, epoch, logs=None):
            if not hasattr(self.model.optimizer, 'lr'):
                raise ValueError('Optimizer must have a "lr" attribute.')
            lr, momentum = self.schedule(epoch)
            K.set_value(self.model.optimizer.lr, lr)
            K.set_value(self.model.optimizer.momentum, momentum)


    batch_size = 128
    nb_classes = 10
    img_rows, img_cols = 28, 28
    nb_filters = 32
    nb_pool = 2
    nb_conv = 5

    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255
    Y_train = np_utils.to_categorical(y_train, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)

    def schedule(x):
        '''Rather ugly way of defining a learning schedule function'''
        x = np.array(x, dtype = 'float32')
        lr = np.piecewise(x, [x < 500, (x >= 500) & (x < 1000), (x >= 1000) & (x < 1500), x >= 1500],
                          [0.01, 0.001, 0.0001, 0.00001])
        momentum = np.piecewise(x, [x < 500, x >= 500],
                                [0.9, 0.99])
        return((lr, momentum))

    frames = []
    optimizer_scheduler = OptimizerScheduler(schedule)
    unif = True 
    epochs = 2000

    frames = []
    for just_dropout, hbp in zip([True, False], [0.65, 0.45]):
        for n in np.exp2(range(10)).astype('int32'):
            small_indices = np.array([], dtype = 'int32')
            for i in range(nb_classes):
                first_n_i_indices = np.arange(X_train.shape[0])[y_train == i][0:n]
                small_indices = np.concatenate([small_indices, first_n_i_indices])

            small_indices = np.tile(small_indices, np.ceil(1000. / small_indices.shape[0]).astype('int32'))
            small_X_train = X_train[small_indices]
            small_y_train = y_train[small_indices]
            small_Y_train = Y_train[small_indices]
            uncorrupted_train_history = UncorruptedTrainHistory([small_X_train, small_Y_train])

            model = Sequential()
            model.add(HB_2d_conv(hbp / 2, shift = -1, input_shape = (img_rows, img_cols, 1), unif = unif, just_dropout = just_dropout))
            model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
            model.add(HB_2d_conv(hbp, shift = -2, unif = unif, just_dropout = just_dropout))
            model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
            model.add(MaxPooling2D((nb_pool, nb_pool)))
            model.add(HB_2d_conv(hbp, shift = -3, unif = unif, just_dropout = just_dropout))
            model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
            model.add(HB_2d_conv(hbp, shift = -4, unif = unif, just_dropout = just_dropout))
            model.add(Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'valid', activation = 'relu'))
            model.add(MaxPooling2D((2, 2)))
            model.add(Flatten())
            model.add(HB(hbp, shift = -6, unif = unif, just_dropout = just_dropout))
            model.add(Dense(nb_classes, activation = 'softmax'))
            sgd = SGD(lr = 0.01, momentum = 0.9, decay = 0.00, nesterov = False)
            model.compile(loss='categorical_crossentropy',
                          optimizer= sgd,
                          metrics=['accuracy'])
            myfit = model.fit(small_X_train, small_Y_train, batch_size = batch_size, epochs = epochs,
                              verbose = 1, validation_data=(X_test, Y_test), callbacks = [optimizer_scheduler, uncorrupted_train_history])

            df = pd.DataFrame.from_dict(myfit.history)
            df['uncorrupted_train_acc'] = uncorrupted_train_history.accs
            df['uncorrupted_train_loss'] = uncorrupted_train_history.losses
            df['hbp'] = np.repeat(hbp, epochs)
            df['uniform'] = np.repeat(unif, epochs)
            df['just_dropout'] = np.repeat(just_dropout, epochs)
            df['n'] = np.repeat(n, epochs)
            frames.append(df)

    out_frame = pd.concat(frames)
    out_frame.to_csv('./data/convergence_curves.csv')
  #+end_src 

  #+RESULTS[ab3559fa092d4232a3feac65bd41837b4968865d]: accuracy_vs_n_data_generator
  : None

  We find the hybrid bootstrap to be particularly effective when only
  a small number of training points are available.  In the most
  extreme case, only one training point per class exists.  So-called
  one-shot learning seeks to discriminate based on a single training
  example.  In Figure [[perf_vs_size_fig]], we compare the performance of
  dropout and the hybrid bootstrap for different training set sizes
  using the hyperparameters $u=0.45$ and $u = 0.65$ for the hybrid
  bootstrap and dropout respectively.
  #+name: perf_vs_size_fig
  #+begin_src R :exports results :file convergence_figure.pdf
  library(dplyr)
  library(ggplot2)
  library(gridExtra)
  library(tidyr)
  data <- read.csv("./data/convergence_curves.csv")
  final_values <- data[data$X == 1999,]
  final_values$just_dropout <- factor(final_values$just_dropout,
                                      levels = c("False", "True"),
                                      labels = c("Hybrid\nBootstrap", "Dropout"))
  colnames(final_values)[colnames(final_values) == "just_dropout"] <- "Method"
  long_final_errors <- gather(data = final_values, Metric, accuracy, c(acc, val_acc, uncorrupted_train_acc))
  long_final_losses <- gather(data = final_values, Metric, loss, c(loss, val_loss, uncorrupted_train_loss))
  long_final_errors$Metric <- factor(long_final_errors$Metric, 
                                         levels = unique(long_final_errors$Metric),
                                         labels = c("Training\nError\n",
                                                    "Test\nError\n",
                                                    "Uncorrupted\nTraining\nError\n"))
  long_final_losses$Metric <- factor(long_final_losses$Metric, 
                                         levels = unique(long_final_losses$Metric),
                                         labels = c("Training\nCCE\n",
                                                    "Test\nCCE\n",
                                                    "Uncorrupted\nTraining\nCCE\n"))
                                         
  acc_plot <- ggplot(data = long_final_errors, aes(x = n, y = 1 - accuracy, color = Method)) +
    geom_line(aes(linetype = Metric)) +
    geom_point(aes(shape = Metric)) + 
    scale_x_continuous(trans = "log2", breaks = 2 ^ (0:9), minor_breaks = NULL) + 
    scale_y_continuous(minor_breaks = seq(0, 1, 0.01), breaks = seq(0, 1, 0.05)) +
    xlab(expression(n/k)) + 
    ylab("Misclassification Rate") + 
    theme(legend.text = element_text(size = 8), legend.title = element_text(size = 8))

  loss_plot <- ggplot(data = long_final_losses, aes(x = n, y = loss, color = Method)) +
    geom_line(aes(linetype = Metric)) +
    geom_point(aes(shape = Metric)) + 
    scale_x_continuous(trans = "log2", breaks = 2 ^ (0:9), minor_breaks = NULL) +
    xlab(expression(n/k)) + 
    ylab("Logloss") + 
    geom_hline(yintercept = -log(0.1)) + 
    annotate("label", x = 2^7, y = -log(0.1) + 0.1,
             label = "Random Guess", size = 2)  + 
    theme(legend.text = element_text(size = 8), legend.title = element_text(size = 8))

  log_acc_plot <- ggplot(data = final_values, aes(x = n, y = 1 - val_acc, color = Method)) +
    geom_line() +
    geom_point() + 
    scale_x_continuous(trans = "log2", breaks = 2 ^ (0:9), minor_breaks = NULL) + 
    scale_y_continuous(trans = "log2", minor_breaks = seq(0, 1, 0.01), breaks = seq(0, 1, 0.05)) +
    xlab(expression(n/k)) + 
    ylab("Test Set Misclassification Rate") + 
    theme(legend.text = element_text(size = 8), legend.title = element_text(size = 8))

  log_loss_plot <- ggplot(data = final_values, aes(x = n, y = val_loss, color = Method)) +
    geom_line() +
    geom_point() + 
    scale_x_continuous(trans = "log2", breaks = 2 ^ (0:9), minor_breaks = NULL) + 
    scale_y_continuous(trans = "log2") +
    xlab(expression(n/k)) + 
    ylab("Test Logloss")+ 
    geom_hline(yintercept = -log(0.1)) + 
    annotate("label", x = 2^7, y = -log(0.1) + 0.3,
             label = "Random Guess", size = 2) + 
    theme(legend.text = element_text(size = 8), legend.title = element_text(size = 8))

  plot <- grid.arrange(acc_plot, log_acc_plot, loss_plot, log_loss_plot, ncol = 2)


  ggsave("convergence_figure.pdf",
             plot,
             device = "pdf",
             width = 8.5,
             height = 6.5,
         units = "in")
  #+end_src
  #+caption: Performance of the hybrid bootstrap compared to dropout for different training set sizes.
  #+caption: Here a random guess assigns a probability of $\frac{1}{k}$ where $k$ is the number of classes.
  #+label: perf_vs_size_fig
  #+attr_latex: :placement [!htpb]
  #+RESULTS: perf_vs_size_fig  
  [[file:convergence_figure.pdf]]

  Both techniques perform remarkably well even for small dataset sizes
  but the hybrid bootstrap has a clear advantage.  If one considers
  the logloss as a measure of model performance, the hybrid bootstrap
  works even when only one or two examples from each class are
  available.  However, dropout is less effective than assigning equal
  odds to each class for those dataset sizes.  The error rate of the
  network on dropout-corrupted data (shown in the top left panel of
  Figure [[perf_vs_size_fig]]) is quite low even though there is a large
  amount of dropout.  This comparison is potentially unfair to dropout
  as an experienced practitioner may suspect that our test
  architecture contains too many parameters for such a small training
  set before using it.  However, for the less-experienced who must rely on it, cross
  validation is challenging with only one training point.

* Benchmarks
  <<benchmarks>> The previous sections employed smaller versions of
  the MNIST training digits for the sake of speed, but clearly the
  hybrid bootstrap is only useful if it works for larger datasets and
  for data besides the MNIST digits.  To evaluate the hybrid
  bootstrap's performance on three standard image benchmarks, we adopt
  a gls:cnn architecture very similar to the glspl:wrn of Zagoruyko and
  Komodakis cite:Zagoruyko2016WRN with three major differences.
  First, they applied dropout immediately prior to certain weight
  layers.  Since their network uses skip connections, this means
  difficult regularization patterns can be bypassed, defeating the
  regularization.  We therefore apply the hybrid bootstrap prior to
  each set of network blocks at a particular resolution.  Second, we
  use 160 rather than 16 filters in the initial convolutional layer.
  This allows us to use the same level of hybrid bootstrap for each of
  the three regularization layers. Third, their training schedule
  halted after decreasing the learning rate three times by 80\%.  Our
  version of the network continues to improve significantly at lower
  learning rates, so we decrease by the same amount five times. Our
  architecture is visualized in Figure [[benchmark_architecture]].
  #+name: benchmark_architecture
  #+begin_src python :exports results :results file
    from graphviz import Digraph

    # It would be better if this was generated by the actual code using
    # a parser

    my_graph = Digraph()
    my_graph.attr(ranksep = '0.2')
    my_graph.attr(nodesep = '0.5')
    my_graph.attr('graph', size = '6.5, 6.5', margin = '0')
    my_graph.attr('node', shape = 'rect')
    my_graph.attr('edge', weight = '2')


    my_graph.node('0', 'Image', style = 'filled', fillcolor = 'grey')
    my_graph.node('1', 'Convolution Layer, 160 3x3 Filters', style = 'filled', fillcolor = 'blanchedalmond')
    my_graph.node('2', 'Spatial Grid Hybrid Bootstrap, u = 0.45', style = 'filled', fillcolor = 'forestgreen')
    my_graph.node('3', 'Residual Junction Block, filters = 160', style = 'filled', fillcolor = 'white')
    my_graph.node('4', '<[Residual Block, filters = 160]<sup>3</sup>>', style = 'filled', fillcolor = 'white')
    my_graph.node('5', 'Spatial Grid Hybrid Bootstrap, u = 0.45', style = 'filled', fillcolor = 'forestgreen')
    my_graph.node('6', 'Residual Junction Block, filters = 320', style = 'filled', fillcolor = 'white')
    my_graph.node('7', '<[Wide Residual Block, filters = 320]<sup>3</sup>>', style = 'filled', fillcolor = 'white')
    my_graph.node('8', 'Spatial Grid Hybrid Bootstrap, u = 0.45', style = 'filled', fillcolor = 'forestgreen')
    my_graph.node('9', 'Wide Residual Junction Block, filters = 640', style = 'filled', fillcolor = 'white')
    my_graph.node('10', '<[Wide Residual Block, filters = 640]<sup>3</sup>>', style = 'filled', fillcolor = 'white')
    my_graph.node('11', 'Spatial Batch Normalization', style = 'filled', fillcolor = 'blueviolet')
    my_graph.node('12', 'ReLU Activation', style = 'filled', fillcolor = 'red')
    my_graph.node('13', 'Spatial Global Average Pooling', style = 'filled', fillcolor = 'deepskyblue1')
    my_graph.node('14', 'Softmax Layer', style = 'filled', fillcolor = 'blanchedalmond')
    my_graph.node('15', 'Class Probabilities', style = 'filled', fillcolor = 'grey')

    for i in range(15):
        my_graph.edge(str(i), str(i + 1))

    my_graph.node('24', 'Residual Block, filters = #filters', style = 'filled', fillcolor = 'white')
    my_graph.node('25', 'Input', style = 'filled', fillcolor = 'grey')
    my_graph.node('26', 'Spatial Batch Normalization', style = 'filled', fillcolor = 'blueviolet')
    my_graph.node('27', 'ReLU Activation', style = 'filled', fillcolor = 'red')
    my_graph.node('28', 'Convolution Layer, #filters 3x3 Filters', style = 'filled', fillcolor = 'blanchedalmond')
    my_graph.node('29', 'Spatial Batch Normalization', style = 'filled', fillcolor = 'blueviolet')
    my_graph.node('30', 'ReLU Activation', style = 'filled', fillcolor = 'red')
    my_graph.node('31', 'Convolution Layer, #filters 3x3 Filters', style = 'filled', fillcolor = 'blanchedalmond')
    my_graph.node('32', '+', style = 'filled', fillcolor = 'white', shape = 'circle')


    for i in range(25, 32):
        my_graph.edge(str(i), str(i + 1))

    my_graph.edge(str(24), str(25), style = 'invis')
    my_graph.edge(str(25), str(32))

    my_graph.node('33', 'Residual Junction Block, filters = #filters', style = 'filled', fillcolor = 'white')
    my_graph.node('34', 'Input', style = 'filled', fillcolor = 'grey')
    my_graph.node('35', 'Spatial Batch Normalization', style = 'filled', fillcolor = 'blueviolet')
    my_graph.node('36', 'ReLU Activation', style = 'filled', fillcolor = 'red')
    my_graph.node('37', 'Convolution Layer, #filters 1x1 Filters, Stride = 2', style = 'filled', fillcolor = 'blanchedalmond')
    my_graph.node('38', 'Convolution Layer, #filters 3x3 Filters, Stride = 2', style = 'filled', fillcolor = 'blanchedalmond')
    my_graph.node('39', 'Spatial Batch Normalization', style = 'filled', fillcolor = 'blueviolet')
    my_graph.node('40', 'ReLU Activation', style = 'filled', fillcolor = 'red')
    my_graph.node('41', 'Convolution Layer, #filters 3x3 Filters', style = 'filled', fillcolor = 'blanchedalmond')
    my_graph.node('42', '+', style = 'filled', fillcolor = 'white', shape = 'circle')



    my_graph.edge(str(33), str(34), style = 'invis')
    my_graph.edge(str(34), str(35))
    my_graph.edge(str(35), str(36))
    my_graph.edge(str(36), str(37))
    my_graph.edge(str(36), str(38))
    for i in range(38, 42):
        my_graph.edge(str(i), str(i + 1))

    my_graph.edge(str(37), str(42))

    my_graph.render('./benchmark_network', view = False)
    return './benchmark_network.pdf'
  #+end_src
  #+caption: Network architecture for benchmark results.  Biases are not used.  All convolutional layers have linear activations.
  #+caption:  Exponents represent the number of repeated layers.
  #+label: benchmark_architecture
  #+attr_latex: :width 0.9\textwidth
  #+RESULTS: benchmark_architecture
  [[file:./benchmark_network.pdf]] 

  We test this network on the CIFAR10 and CIFAR100 datasets, which
  consist of RGB images with 50,000 training examples and 10,000 test
  cases each and 10 and 100 classes respectively
  cite:krizhevsky2009learning.  We also evaluate this network on the
  MNIST digits. We augment the CIFAR data with 15\% translations and
  horizontal flips.  We do not use data augmentation for the MNIST
  digits. The images are preprocessed by centering and scaling
  according to the channel-wise mean and standard deviation of the
  training data.  We use gls:sgd with Nesterov momentum 0.9 and start
  with learning rate 0.1.  The learning rate is decreased by 80\%
  every 60 epochs and the network is trained for 360 epochs total.
  The results are given in Table [[benchmark_table]].  We attempted to use
  dropout in the same position as we use the hybrid bootstrap, but
  this worked very poorly.  At dropout levels $p = 0.5$ and $p =
  0.25$, the misclassification rates on the CIFAR100 test set are
  src_python[:exports results :var x = wide_resnet_dropout_cifar100_bench :results raw]{return(x)}\% and
  src_python[:exports results :var x = wide_resnet_less_dropout_cifar100_bench :results raw]{return(x)}\%
  respectively, which is much worse than the hybrid bootstrap result.  To
  have a real comparison to dropout, we have included the
  dropout-based results from the original wide residual network paper.
  It is apparent in Table [[benchmark_table]] that adding the hybrid
  bootstrap in our location makes a much bigger difference than adding
  dropout to the residual blocks.

  #+name: channels_first_hb_conv
  #+begin_src python :exports none
    import numpy as np
    from keras import backend as K
    from theano import tensor as T
    from keras.engine.topology import Layer
    from keras.initializers import Constant

    # Theano has much faster batch normalization if
    # data format is channels first.

    def hybo_2d_conv(x, p, shift, seed = None, unif = True, just_dropout = False):
        '''Theano hybrid bootstrap backend'''
        if p.get_value() < 0. or p.get_value() > 1:
            raise Exception('Hybrid bootstrap p must be in interval [0, 1].')
        if seed is None:
            seed = np.random.randint(1, 10e6)
            rng = K.RandomStreams(seed = seed)
        if(unif == True):
            retain_prob = 1. - rng.uniform((x.shape[0],), 0, p, dtype = x.dtype)
            for dim in range(x.ndim - 1):
                retain_prob = K.expand_dims(retain_prob, dim + 1)
        else:
            retain_prob = 1. - p
        mask = rng.binomial((x.shape[0], 1, x.shape[2], x.shape[3]), p=retain_prob, dtype=x.dtype)
        mask = T.extra_ops.repeat(mask, x.shape[1], axis = 1)
        if just_dropout:
            x = x * mask / retain_prob
        else:
            x = x * mask + (1 - mask) * T.roll(x, shift = shift, axis = 0)
        return x



    class HB_2d_conv(Layer):
        '''Applies the hybrid bootstrap to the input.
            # Arguments
            p: float between 0 and 1. Fraction of the input units to resample if unif = F,
               maximum fraction if unif = T
            shift: int. Should be smaller than batch size.
            unif: bool.  Should p be sampled from unif(0, p)?
            just_dropout: Should we just do dropout?
            '''
        def __init__(self, p, shift, unif = True, just_dropout = False, **kwargs):
            self.init_p = p
            self.shift = shift
            self.unif = unif
            self.just_dropout = just_dropout
            self.uses_learning_phase = True
            self.supports_masking = True
            super(HB_2d_conv, self).__init__(**kwargs)
        def build(self, input_shape):
            self.p = self.add_weight(shape=(),
                                     name = 'p',
                                     initializer=Constant(value = self.init_p),
                                     trainable=False)
            super(HB_2d_conv, self).build(input_shape)
        def call(self, x, mask=None):
            if 0. < self.p.get_value() < 1.:
                x = K.in_train_phase(hybo_2d_conv(x, p = self.p, shift = self.shift,
                                          unif = self.unif,
                                          just_dropout = self.just_dropout), x)
            return x
        def get_config(self):
            config = {'init_p': self.init_p, 'p': self.p, 'shift': self.shift,
                      'unif': self.unif, 'just_dropout': self.just_dropout}
            base_config = super(HB_2d_conv, self).get_config()
            return dict(list(base_config.items()) + list(config.items()))

  #+end_src
  #+name:wide_resnet_cifar100_bench
  #+begin_src python :exports none :noweb yes :cache yes :results value
    import numpy as np
    np.random.seed(42)  # for reproducibility
    from keras.layers import Dense, Flatten
    from keras.layers.convolutional import Conv2D
    from keras.layers.pooling import AveragePooling2D
    from keras.utils import np_utils
    from keras.optimizers import SGD
    from keras.preprocessing.image import ImageDataGenerator
    from keras.layers.normalization import BatchNormalization
    from keras.preprocessing.image import ImageDataGenerator
    from keras.models import Model
    from keras.layers import Input
    from keras.layers.merge import add, concatenate
    from keras import regularizers
    from keras import backend as K
    from keras.layers import Lambda
    from keras.datasets import cifar100
    from keras.callbacks import LearningRateScheduler

    <<channels_first_hb_conv>>

    def schedule(x):
        x = np.array(x, dtype = 'float32')
        lr = np.piecewise(x,[x <= 60,
                             (x > 60) & (x <= 120),
                             (x > 120) & (x <= 180),
                             (x > 180) & (x <= 240),
                             (x > 240) & (x <= 300),
                             x > 300],
                          [0.1,
                           0.1 * 0.2,
                           0.1 * 0.2 ** 2,
                           0.1 * 0.2 ** 3,
                           0.1 * 0.2 ** 4,
                           0.1 * 0.2 ** 5])
        return(float(lr))

    optimizer_schedule = LearningRateScheduler(schedule)
    batch_size = 128
    nb_epoch = 360
    hbp = 0.45
    wd = 0.0005
    nb_filters = 160
    nb_conv = 3

    (X_train_s, y_train_s), (X_test, y_test) = cifar100.load_data()
    img_rows = X_train_s.shape[1]
    img_cols = X_train_s.shape[2]
    nb_classes = np.max(y_train_s) + 1

    train_mean = np.mean(X_train_s, axis = (0, 1, 2)).reshape((1, 1, 1, 3))
    train_sd = np.std(X_train_s, axis = (0, 1, 2)).reshape((1, 1, 1, 3))

    X_train_s = (X_train_s - train_mean) / train_sd
    X_test = (X_test - train_mean) / train_sd

    # Change to channels first format for Theano
    X_train_s = np.moveaxis(X_train_s, [1, 2, 3], [2, 3, 1])
    X_test = np.moveaxis(X_test, [1, 2, 3], [2, 3, 1])

    X_train_s = X_train_s.astype('float32')
    X_test = X_test.astype('float32')

    # convert class vectors to binary class matrices
    Y_train_s = np_utils.to_categorical(y_train_s, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)

    def rectifier(x):
        return K.relu(x)

    def build_resnet_block(inp, nb_reps = 1, nb_conv = 3, nb_filters = 32,
                           hbp = 0.01, wd = 0.0001, stride = 1,
                           data_format = None):
        through = BatchNormalization(axis = 1)(inp)
        through = Lambda(rectifier)(through)
        skip = Conv2D(nb_filters, kernel_size = (1, 1), padding = 'same',
                      strides = (stride, stride), kernel_regularizer = regularizers.l2(wd),
                      use_bias = False, data_format = data_format)(through)
        through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                         padding = 'same',
                         strides = (stride, stride),
                         kernel_regularizer = regularizers.l2(wd),
                         use_bias = False, data_format = data_format)(through)
        through = BatchNormalization(axis = 1)(through)
        through = Lambda(rectifier)(through)
        through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                         padding = 'same',
                         strides = (1, 1), kernel_regularizer = regularizers.l2(wd),
                         use_bias = False, data_format = data_format)(through)
        inp = add([through, skip])
        for i in range(nb_reps):
            through = BatchNormalization(axis = 1)(inp)
            through = Lambda(rectifier)(through)
            through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'same',
                             kernel_regularizer = regularizers.l2(wd),
                             use_bias = False, data_format = data_format)(through)
            through = BatchNormalization(axis = 1)(through)
            through = Lambda(rectifier)(through)
            through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                             padding = 'same', kernel_regularizer = regularizers.l2(wd),
                             use_bias = False, data_format = data_format)(through)
            inp = add([through, inp])
        return(inp)

    data_format = "channels_first"

    inp = Input(shape=(3, img_rows, img_cols))
    exp = Conv2D(filters = 160,
                 kernel_size = (nb_conv, nb_conv),
                 padding = 'same',
                 kernel_regularizer = regularizers.l2(wd),
                 data_format = data_format,
                 use_bias = False)(inp)
    hb1 = HB_2d_conv(hbp, shift = -1)(exp)
    block1 = build_resnet_block(hb1, nb_reps = 3, nb_conv = nb_conv,
                                nb_filters = nb_filters,
                                hbp = hbp, wd = wd, data_format = data_format)
    hb2 = HB_2d_conv(hbp, shift = -2)(block1)
    block2 = build_resnet_block(hb2, nb_reps = 3, nb_conv = nb_conv,
                                nb_filters = 2 * nb_filters,
                                hbp = hbp, wd = wd, stride = 2,
                                data_format = data_format)
    hb3 = HB_2d_conv(hbp, shift = -3)(block2)
    block3 = build_resnet_block(hb3, nb_reps = 3,
                                nb_conv = nb_conv, nb_filters = 4 * nb_filters,
                                hbp = hbp, wd = wd, stride = 2,
                                data_format = data_format)
    bn_pre_pool = BatchNormalization(axis = 1)(block3)
    rectifier_pre_pool = Lambda(rectifier)(bn_pre_pool)
    pool = AveragePooling2D(pool_size=(8, 8), data_format = data_format)(rectifier_pre_pool)
    flat = Flatten()(pool)
    dense = Dense(nb_classes, activation = 'softmax',
                   kernel_regularizer = regularizers.l2(wd),
                  use_bias = False)(flat)
    model = Model(inputs = inp, outputs = dense)

    datagen = ImageDataGenerator(width_shift_range = 0.15,
                                 height_shift_range = 0.15,
                                 zoom_range = 0.0,
                                 rotation_range = 0,
                                 horizontal_flip = True,
                                 fill_mode='reflect',
                                 data_format = data_format)

    sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0.00, nesterov = True)
    model.compile(loss = 'categorical_crossentropy',
                  optimizer = sgd,
                  metrics=['accuracy'])

    fit = model.fit_generator(datagen.flow(X_train_s, Y_train_s,
                                           batch_size = batch_size),
                              steps_per_epoch = np.ceil(1. * X_train_s.shape[0] / batch_size),
                              epochs = 360,
                              verbose = 1, validation_data = (X_test, Y_test),
                              callbacks = [optimizer_schedule])

    return(format(100 * (1 - fit.history['val_acc'][-1]), '.2f'))
  #+end_src

  #+RESULTS[da46a2f06789207e800b767c8049030d1a786154]: wide_resnet_cifar100_bench
  : 18.36

  #+name:wide_resnet_cifar10_bench
  #+begin_src python :exports none :noweb yes :cache yes :results value
    import numpy as np
    np.random.seed(42)  # for reproducibility
    from keras.layers import Dense, Flatten
    from keras.layers.convolutional import Conv2D
    from keras.layers.pooling import AveragePooling2D
    from keras.utils import np_utils
    from keras.optimizers import SGD
    from keras.preprocessing.image import ImageDataGenerator
    from keras.layers.normalization import BatchNormalization
    from keras.preprocessing.image import ImageDataGenerator
    from keras.models import Model
    from keras.layers import Input
    from keras.layers.merge import add, concatenate
    from keras import regularizers
    from keras import backend as K
    from keras.layers import Lambda
    from keras.datasets import cifar10
    from keras.callbacks import LearningRateScheduler

    <<channels_first_hb_conv>>

    def schedule(x):
        x = np.array(x, dtype = 'float32')
        lr = np.piecewise(x,[x <= 60,
                             (x > 60) & (x <= 120),
                             (x > 120) & (x <= 180),
                             (x > 180) & (x <= 240),
                             (x > 240) & (x <= 300),
                             x > 300],
                          [0.1,
                           0.1 * 0.2,
                           0.1 * 0.2 ** 2,
                           0.1 * 0.2 ** 3,
                           0.1 * 0.2 ** 4,
                           0.1 * 0.2 ** 5])
        return(float(lr))

    optimizer_schedule = LearningRateScheduler(schedule)
    batch_size = 128
    nb_epoch = 360
    hbp = 0.45
    wd = 0.0005
    nb_filters = 160
    nb_conv = 3

    (X_train_s, y_train_s), (X_test, y_test) = cifar10.load_data()
    img_rows = X_train_s.shape[1]
    img_cols = X_train_s.shape[2]
    nb_classes = np.max(y_train_s) + 1

    train_mean = np.mean(X_train_s, axis = (0, 1, 2)).reshape((1, 1, 1, 3))
    train_sd = np.std(X_train_s, axis = (0, 1, 2)).reshape((1, 1, 1, 3))

    X_train_s = (X_train_s - train_mean) / train_sd
    X_test = (X_test - train_mean) / train_sd

    # Change to channels first format for Theano
    X_train_s = np.moveaxis(X_train_s, [1, 2, 3], [2, 3, 1])
    X_test = np.moveaxis(X_test, [1, 2, 3], [2, 3, 1])

    X_train_s = X_train_s.astype('float32')
    X_test = X_test.astype('float32')

    # convert class vectors to binary class matrices
    Y_train_s = np_utils.to_categorical(y_train_s, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)

    def rectifier(x):
        return K.relu(x)

    def build_resnet_block(inp, nb_reps = 1, nb_conv = 3, nb_filters = 32,
                           hbp = 0.01, wd = 0.0001, stride = 1,
                           data_format = None):
        through = BatchNormalization(axis = 1)(inp)
        through = Lambda(rectifier)(through)
        skip = Conv2D(nb_filters, kernel_size = (1, 1), padding = 'same',
                      strides = (stride, stride), kernel_regularizer = regularizers.l2(wd),
                      use_bias = False, data_format = data_format)(through)
        through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                         padding = 'same',
                         strides = (stride, stride),
                         kernel_regularizer = regularizers.l2(wd),
                         use_bias = False, data_format = data_format)(through)
        through = BatchNormalization(axis = 1)(through)
        through = Lambda(rectifier)(through)
        through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                         padding = 'same',
                         strides = (1, 1), kernel_regularizer = regularizers.l2(wd),
                         use_bias = False, data_format = data_format)(through)
        inp = add([through, skip])
        for i in range(nb_reps):
            through = BatchNormalization(axis = 1)(inp)
            through = Lambda(rectifier)(through)
            through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'same',
                             kernel_regularizer = regularizers.l2(wd),
                             use_bias = False, data_format = data_format)(through)
            through = BatchNormalization(axis = 1)(through)
            through = Lambda(rectifier)(through)
            through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                             padding = 'same', kernel_regularizer = regularizers.l2(wd),
                             use_bias = False, data_format = data_format)(through)
            inp = add([through, inp])
        return(inp)

    data_format = "channels_first"

    inp = Input(shape=(3, img_rows, img_cols))
    exp = Conv2D(filters = 160,
                 kernel_size = (nb_conv, nb_conv),
                 padding = 'same',
                 kernel_regularizer = regularizers.l2(wd),
                 data_format = data_format,
                 use_bias = False)(inp)
    hb1 = HB_2d_conv(hbp, shift = -1)(exp)
    block1 = build_resnet_block(hb1, nb_reps = 3, nb_conv = nb_conv,
                                nb_filters = nb_filters,
                                hbp = hbp, wd = wd, data_format = data_format)
    hb2 = HB_2d_conv(hbp, shift = -2)(block1)
    block2 = build_resnet_block(hb2, nb_reps = 3, nb_conv = nb_conv,
                                nb_filters = 2 * nb_filters,
                                hbp = hbp, wd = wd, stride = 2,
                                data_format = data_format)
    hb3 = HB_2d_conv(hbp, shift = -3)(block2)
    block3 = build_resnet_block(hb3, nb_reps = 3,
                                nb_conv = nb_conv, nb_filters = 4 * nb_filters,
                                hbp = hbp, wd = wd, stride = 2,
                                data_format = data_format)
    bn_pre_pool = BatchNormalization(axis = 1)(block3)
    rectifier_pre_pool = Lambda(rectifier)(bn_pre_pool)
    pool = AveragePooling2D(pool_size=(8, 8), data_format = data_format)(rectifier_pre_pool)
    flat = Flatten()(pool)
    dense = Dense(nb_classes, activation = 'softmax',
                   kernel_regularizer = regularizers.l2(wd),
                  use_bias = False)(flat)
    model = Model(inputs = inp, outputs = dense)

    datagen = ImageDataGenerator(width_shift_range = 0.15,
                                 height_shift_range = 0.15,
                                 zoom_range = 0.0,
                                 rotation_range = 0,
                                 horizontal_flip = True,
                                 fill_mode='reflect',
                                 data_format = data_format)

    sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0.00, nesterov = True)
    model.compile(loss = 'categorical_crossentropy',
                  optimizer = sgd,
                  metrics=['accuracy'])

    fit = model.fit_generator(datagen.flow(X_train_s, Y_train_s,
                                           batch_size = batch_size),
                              steps_per_epoch = np.ceil(1. * X_train_s.shape[0] / batch_size),
                              epochs = 360,
                              verbose = 1, validation_data = (X_test, Y_test),
                              callbacks = [optimizer_schedule])

    return(format(100 * (1 - fit.history['val_acc'][-1]), '.2f'))
  #+end_src

  #+RESULTS[9fc0245db595f828d08f043a651df93545639195]: wide_resnet_cifar10_bench
  : 3.4

  #+name:wide_resnet_mnist_bench
  #+begin_src python :exports none :noweb yes :cache yes :results value
    import numpy as np
    np.random.seed(42)  # for reproducibility
    from keras.layers import Dense, Flatten
    from keras.layers.convolutional import Conv2D
    from keras.layers.pooling import AveragePooling2D
    from keras.utils import np_utils
    from keras.optimizers import SGD
    from keras.preprocessing.image import ImageDataGenerator
    from keras.layers.normalization import BatchNormalization
    from keras.preprocessing.image import ImageDataGenerator
    from keras.models import Model
    from keras.layers import Input
    from keras.layers.merge import add, concatenate
    from keras import regularizers
    from keras import backend as K
    from keras.layers import Lambda
    from keras.datasets import mnist
    from keras.callbacks import LearningRateScheduler

    <<channels_first_hb_conv>>

    def schedule(x):
               x = np.array(x, dtype = 'float32')
               lr = np.piecewise(x,
                                 [x <= 60,
                                  (x > 60) & (x <= 120),
                                  (x > 120) & (x <= 180),
                                  (x > 180) & (x <= 240),
                                  (x > 240) & (x <= 300),
                                  x > 300],
                                 [0.1,
                                  0.1 * 0.2,
                                  0.1 * 0.2 ** 2,
                                  0.1 * 0.2 ** 3,
                                  0.1 * 0.2 ** 4,
                                  0.1 * 0.2 ** 5])
               return(float(lr))
    optimizer_schedule = LearningRateScheduler(schedule)
    batch_size = 128
    nb_epoch = 360
    hbp = 0.45
    wd = 0.0005
    nb_filters = 160
    nb_conv = 3

    (X_train_s, y_train_s), (X_test, y_test) = mnist.load_data()
    img_rows = X_train_s.shape[1]
    img_cols = X_train_s.shape[2]

    X_train_s = X_train_s.reshape(X_train_s.shape[0], img_rows, img_cols, 1)
    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)

    nb_classes = np.max(y_train_s) + 1

    train_mean = np.mean(X_train_s, axis = (0, 1, 2)).reshape((1, 1, 1, 1))
    train_sd = np.std(X_train_s, axis = (0, 1, 2)).reshape((1, 1, 1, 1))

    X_train_s = (X_train_s - train_mean) / train_sd
    X_test = (X_test - train_mean) / train_sd

    # Change to channels first format for Theano
    X_train_s = np.moveaxis(X_train_s, [1, 2, 3], [2, 3, 1])
    X_test = np.moveaxis(X_test, [1, 2, 3], [2, 3, 1])

    X_train_s = X_train_s.astype('float32')
    X_test = X_test.astype('float32')

    # convert class vectors to binary class matrices
    Y_train_s = np_utils.to_categorical(y_train_s, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)

    def rectifier(x):
        return K.relu(x)

    def build_resnet_block(inp, nb_reps = 1, nb_conv = 3, nb_filters = 32,
                           hbp = 0.01, wd = 0.0001, stride = 1,
                           data_format = None):
        through = BatchNormalization(axis = 1)(inp)
        through = Lambda(rectifier)(through)
        skip = Conv2D(nb_filters, kernel_size = (1, 1), padding = 'same',
                      strides = (stride, stride), kernel_regularizer = regularizers.l2(wd),
                      use_bias = False, data_format = data_format)(through)
        through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                         padding = 'same',
                         strides = (stride, stride),
                         kernel_regularizer = regularizers.l2(wd),
                         use_bias = False, data_format = data_format)(through)
        through = BatchNormalization(axis = 1)(through)
        through = Lambda(rectifier)(through)
        through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                         padding = 'same',
                         strides = (1, 1), kernel_regularizer = regularizers.l2(wd),
                         use_bias = False, data_format = data_format)(through)
        inp = add([through, skip])
        for i in range(nb_reps):
            through = BatchNormalization(axis = 1)(inp)
            through = Lambda(rectifier)(through)
            through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'same',
                             kernel_regularizer = regularizers.l2(wd),
                             use_bias = False, data_format = data_format)(through)
            through = BatchNormalization(axis = 1)(through)
            through = Lambda(rectifier)(through)
            through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                             padding = 'same', kernel_regularizer = regularizers.l2(wd),
                             use_bias = False, data_format = data_format)(through)
            inp = add([through, inp])
        return(inp)

    data_format = "channels_first"

    inp = Input(shape=(1, img_rows, img_cols))
    exp = Conv2D(filters = 160,
                 kernel_size = (nb_conv, nb_conv),
                 padding = 'same',
                 kernel_regularizer = regularizers.l2(wd),
                 data_format = data_format,
                 use_bias = False)(inp)
    hb1 = HB_2d_conv(hbp, shift = -1)(exp)
    block1 = build_resnet_block(hb1, nb_reps = 3, nb_conv = nb_conv,
                                nb_filters = nb_filters,
                                hbp = hbp, wd = wd, data_format = data_format)
    hb2 = HB_2d_conv(hbp, shift = -2)(block1)
    block2 = build_resnet_block(hb2, nb_reps = 3, nb_conv = nb_conv,
                                nb_filters = 2 * nb_filters,
                                hbp = hbp, wd = wd, stride = 2,
                                data_format = data_format)
    hb3 = HB_2d_conv(hbp, shift = -3)(block2)
    block3 = build_resnet_block(hb3, nb_reps = 3,
                                nb_conv = nb_conv, nb_filters = 4 * nb_filters,
                                hbp = hbp, wd = wd, stride = 2,
                                data_format = data_format)
    bn_pre_pool = BatchNormalization(axis = 1)(block3)
    rectifier_pre_pool = Lambda(rectifier)(bn_pre_pool)
    pool = AveragePooling2D(pool_size=(7, 7), data_format = data_format)(rectifier_pre_pool)
    flat = Flatten()(pool)
    dense = Dense(nb_classes, activation = 'softmax',
                   kernel_regularizer = regularizers.l2(wd),
                  use_bias = False)(flat)
    model = Model(inputs = inp, outputs = dense)

    sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0.00, nesterov = True)
    model.compile(loss = 'categorical_crossentropy',
                  optimizer = sgd,
                  metrics=['accuracy'])

    fit = model.fit(X_train_s, Y_train_s,
                    batch_size = batch_size,
                    epochs = 360,
                    verbose = 1, validation_data = (X_test, Y_test),
                    callbacks = [optimizer_schedule])

    return(format(100 * (1 - fit.history['val_acc'][-1]), '.2f'))
  #+end_src

  #+RESULTS[bb75a19140b001ff85adef7f0b0218a9e3de7770]: wide_resnet_mnist_bench
  : 0.3

  #+name:wide_resnet_dropout_cifar100_bench
  #+begin_src python :exports none :noweb yes :cache yes :results value
    import numpy as np
    np.random.seed(42)  # for reproducibility
    from keras.layers import Dense, Flatten
    from keras.layers.convolutional import Conv2D
    from keras.layers.pooling import AveragePooling2D
    from keras.utils import np_utils
    from keras.optimizers import SGD
    from keras.preprocessing.image import ImageDataGenerator
    from keras.layers.normalization import BatchNormalization
    from keras.preprocessing.image import ImageDataGenerator
    from keras.models import Model
    from keras.layers import Input, Dropout
    from keras.layers.merge import add, concatenate
    from keras import regularizers
    from keras import backend as K
    from keras.layers import Lambda
    from keras.datasets import cifar100
    from keras.callbacks import LearningRateScheduler

    <<channels_first_hb_conv>>

    def schedule(x):
        x = np.array(x, dtype = 'float32')
        lr = np.piecewise(x,[x <= 60,
                             (x > 60) & (x <= 120),
                             (x > 120) & (x <= 180),
                             (x > 180) & (x <= 240),
                             (x > 240) & (x <= 300),
                             x > 300],
                          [0.1,
                           0.1 * 0.2,
                           0.1 * 0.2 ** 2,
                           0.1 * 0.2 ** 3,
                           0.1 * 0.2 ** 4,
                           0.1 * 0.2 ** 5])
        return(float(lr))

    optimizer_schedule = LearningRateScheduler(schedule)
    batch_size = 128
    nb_epoch = 360
    hbp = 0.45
    wd = 0.0005
    nb_filters = 160
    nb_conv = 3

    (X_train_s, y_train_s), (X_test, y_test) = cifar100.load_data()
    img_rows = X_train_s.shape[1]
    img_cols = X_train_s.shape[2]
    nb_classes = np.max(y_train_s) + 1

    train_mean = np.mean(X_train_s, axis = (0, 1, 2)).reshape((1, 1, 1, 3))
    train_sd = np.std(X_train_s, axis = (0, 1, 2)).reshape((1, 1, 1, 3))

    X_train_s = (X_train_s - train_mean) / train_sd
    X_test = (X_test - train_mean) / train_sd

    # Change to channels first format for Theano
    X_train_s = np.moveaxis(X_train_s, [1, 2, 3], [2, 3, 1])
    X_test = np.moveaxis(X_test, [1, 2, 3], [2, 3, 1])

    X_train_s = X_train_s.astype('float32')
    X_test = X_test.astype('float32')

    # convert class vectors to binary class matrices
    Y_train_s = np_utils.to_categorical(y_train_s, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)

    def rectifier(x):
        return K.relu(x)

    def build_resnet_block(inp, nb_reps = 1, nb_conv = 3, nb_filters = 32,
                           hbp = 0.01, wd = 0.0001, stride = 1,
                           data_format = None):
        through = BatchNormalization(axis = 1)(inp)
        through = Lambda(rectifier)(through)
        skip = Conv2D(nb_filters, kernel_size = (1, 1), padding = 'same',
                      strides = (stride, stride), kernel_regularizer = regularizers.l2(wd),
                      use_bias = False, data_format = data_format)(through)
        through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                         padding = 'same',
                         strides = (stride, stride),
                         kernel_regularizer = regularizers.l2(wd),
                         use_bias = False, data_format = data_format)(through)
        through = BatchNormalization(axis = 1)(through)
        through = Lambda(rectifier)(through)
        through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                         padding = 'same',
                         strides = (1, 1), kernel_regularizer = regularizers.l2(wd),
                         use_bias = False, data_format = data_format)(through)
        inp = add([through, skip])
        for i in range(nb_reps):
            through = BatchNormalization(axis = 1)(inp)
            through = Lambda(rectifier)(through)
            through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'same',
                             kernel_regularizer = regularizers.l2(wd),
                             use_bias = False, data_format = data_format)(through)
            through = BatchNormalization(axis = 1)(through)
            through = Lambda(rectifier)(through)
            through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                             padding = 'same', kernel_regularizer = regularizers.l2(wd),
                             use_bias = False, data_format = data_format)(through)
            inp = add([through, inp])
        return(inp)

    data_format = "channels_first"

    inp = Input(shape=(3, img_rows, img_cols))
    exp = Conv2D(filters = 160,
                 kernel_size = (nb_conv, nb_conv),
                 padding = 'same',
                 kernel_regularizer = regularizers.l2(wd),
                 data_format = data_format,
                 use_bias = False)(inp)
    drop1 = Dropout(0.5)(exp)
    block1 = build_resnet_block(drop1, nb_reps = 3, nb_conv = nb_conv,
                                nb_filters = nb_filters,
                                hbp = hbp, wd = wd, data_format = data_format)
    drop2 = Dropout(0.5)(block1)
    block2 = build_resnet_block(drop2, nb_reps = 3, nb_conv = nb_conv,
                                nb_filters = 2 * nb_filters,
                                hbp = hbp, wd = wd, stride = 2,
                                data_format = data_format)
    drop3 = Dropout(0.5)(block2)
    block3 = build_resnet_block(drop3, nb_reps = 3,
                                nb_conv = nb_conv, nb_filters = 4 * nb_filters,
                                hbp = hbp, wd = wd, stride = 2,
                                data_format = data_format)
    bn_pre_pool = BatchNormalization(axis = 1)(block3)
    rectifier_pre_pool = Lambda(rectifier)(bn_pre_pool)
    pool = AveragePooling2D(pool_size=(8, 8), data_format = data_format)(rectifier_pre_pool)
    flat = Flatten()(pool)
    dense = Dense(nb_classes, activation = 'softmax',
                   kernel_regularizer = regularizers.l2(wd),
                  use_bias = False)(flat)
    model = Model(inputs = inp, outputs = dense)

    datagen = ImageDataGenerator(width_shift_range = 0.15,
                                 height_shift_range = 0.15,
                                 zoom_range = 0.0,
                                 rotation_range = 0,
                                 horizontal_flip = True,
                                 fill_mode='reflect',
                                 data_format = data_format)

    sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0.00, nesterov = True)
    model.compile(loss = 'categorical_crossentropy',
                  optimizer = sgd,
                  metrics=['accuracy'])

    fit = model.fit_generator(datagen.flow(X_train_s, Y_train_s,
                                           batch_size = batch_size),
                              steps_per_epoch = np.ceil(1. * X_train_s.shape[0] / batch_size),
                              epochs = 360,
                              verbose = 1, validation_data = (X_test, Y_test),
                              callbacks = [optimizer_schedule])

    return(format(100 * (1 - fit.history['val_acc'][-1]), '.2f'))
  #+end_src

  #+RESULTS[4dc86460558ed3bb550f7cc2ccbd103d9ab82501]: wide_resnet_dropout_cifar100_bench
  : 50.56

  #+name:wide_resnet_less_dropout_cifar100_bench
  #+begin_src python :exports none :noweb yes :cache yes :results value
    import numpy as np
    np.random.seed(42)  # for reproducibility
    from keras.layers import Dense, Flatten
    from keras.layers.convolutional import Conv2D
    from keras.layers.pooling import AveragePooling2D
    from keras.utils import np_utils
    from keras.optimizers import SGD
    from keras.preprocessing.image import ImageDataGenerator
    from keras.layers.normalization import BatchNormalization
    from keras.preprocessing.image import ImageDataGenerator
    from keras.models import Model
    from keras.layers import Input, Dropout
    from keras.layers.merge import add, concatenate
    from keras import regularizers
    from keras import backend as K
    from keras.layers import Lambda
    from keras.datasets import cifar100
    from keras.callbacks import LearningRateScheduler

    <<channels_first_hb_conv>>

    def schedule(x):
        x = np.array(x, dtype = 'float32')
        lr = np.piecewise(x,[x <= 60,
                             (x > 60) & (x <= 120),
                             (x > 120) & (x <= 180),
                             (x > 180) & (x <= 240),
                             (x > 240) & (x <= 300),
                             x > 300],
                          [0.1,
                           0.1 * 0.2,
                           0.1 * 0.2 ** 2,
                           0.1 * 0.2 ** 3,
                           0.1 * 0.2 ** 4,
                           0.1 * 0.2 ** 5])
        return(float(lr))

    optimizer_schedule = LearningRateScheduler(schedule)
    batch_size = 128
    nb_epoch = 360
    hbp = 0.45
    wd = 0.0005
    nb_filters = 160
    nb_conv = 3

    (X_train_s, y_train_s), (X_test, y_test) = cifar100.load_data()
    img_rows = X_train_s.shape[1]
    img_cols = X_train_s.shape[2]
    nb_classes = np.max(y_train_s) + 1

    train_mean = np.mean(X_train_s, axis = (0, 1, 2)).reshape((1, 1, 1, 3))
    train_sd = np.std(X_train_s, axis = (0, 1, 2)).reshape((1, 1, 1, 3))

    X_train_s = (X_train_s - train_mean) / train_sd
    X_test = (X_test - train_mean) / train_sd

    # Change to channels first format for Theano
    X_train_s = np.moveaxis(X_train_s, [1, 2, 3], [2, 3, 1])
    X_test = np.moveaxis(X_test, [1, 2, 3], [2, 3, 1])

    X_train_s = X_train_s.astype('float32')
    X_test = X_test.astype('float32')

    # convert class vectors to binary class matrices
    Y_train_s = np_utils.to_categorical(y_train_s, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)

    def rectifier(x):
        return K.relu(x)

    def build_resnet_block(inp, nb_reps = 1, nb_conv = 3, nb_filters = 32,
                           hbp = 0.01, wd = 0.0001, stride = 1,
                           data_format = None):
        through = BatchNormalization(axis = 1)(inp)
        through = Lambda(rectifier)(through)
        skip = Conv2D(nb_filters, kernel_size = (1, 1), padding = 'same',
                      strides = (stride, stride), kernel_regularizer = regularizers.l2(wd),
                      use_bias = False, data_format = data_format)(through)
        through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                         padding = 'same',
                         strides = (stride, stride),
                         kernel_regularizer = regularizers.l2(wd),
                         use_bias = False, data_format = data_format)(through)
        through = BatchNormalization(axis = 1)(through)
        through = Lambda(rectifier)(through)
        through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                         padding = 'same',
                         strides = (1, 1), kernel_regularizer = regularizers.l2(wd),
                         use_bias = False, data_format = data_format)(through)
        inp = add([through, skip])
        for i in range(nb_reps):
            through = BatchNormalization(axis = 1)(inp)
            through = Lambda(rectifier)(through)
            through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'same',
                             kernel_regularizer = regularizers.l2(wd),
                             use_bias = False, data_format = data_format)(through)
            through = BatchNormalization(axis = 1)(through)
            through = Lambda(rectifier)(through)
            through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                             padding = 'same', kernel_regularizer = regularizers.l2(wd),
                             use_bias = False, data_format = data_format)(through)
            inp = add([through, inp])
        return(inp)

    data_format = "channels_first"

    inp = Input(shape=(3, img_rows, img_cols))
    exp = Conv2D(filters = 160,
                 kernel_size = (nb_conv, nb_conv),
                 padding = 'same',
                 kernel_regularizer = regularizers.l2(wd),
                 data_format = data_format,
                 use_bias = False)(inp)
    drop1 = Dropout(0.25)(exp)
    block1 = build_resnet_block(drop1, nb_reps = 3, nb_conv = nb_conv,
                                nb_filters = nb_filters,
                                hbp = hbp, wd = wd, data_format = data_format)
    drop2 = Dropout(0.25)(block1)
    block2 = build_resnet_block(drop2, nb_reps = 3, nb_conv = nb_conv,
                                nb_filters = 2 * nb_filters,
                                hbp = hbp, wd = wd, stride = 2,
                                data_format = data_format)
    drop3 = Dropout(0.25)(block2)
    block3 = build_resnet_block(drop3, nb_reps = 3,
                                nb_conv = nb_conv, nb_filters = 4 * nb_filters,
                                hbp = hbp, wd = wd, stride = 2,
                                data_format = data_format)
    bn_pre_pool = BatchNormalization(axis = 1)(block3)
    rectifier_pre_pool = Lambda(rectifier)(bn_pre_pool)
    pool = AveragePooling2D(pool_size=(8, 8), data_format = data_format)(rectifier_pre_pool)
    flat = Flatten()(pool)
    dense = Dense(nb_classes, activation = 'softmax',
                   kernel_regularizer = regularizers.l2(wd),
                  use_bias = False)(flat)
    model = Model(inputs = inp, outputs = dense)

    datagen = ImageDataGenerator(width_shift_range = 0.15,
                                 height_shift_range = 0.15,
                                 zoom_range = 0.0,
                                 rotation_range = 0,
                                 horizontal_flip = True,
                                 fill_mode='reflect',
                                 data_format = data_format)

    sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0.00, nesterov = True)
    model.compile(loss = 'categorical_crossentropy',
                  optimizer = sgd,
                  metrics=['accuracy'])

    fit = model.fit_generator(datagen.flow(X_train_s, Y_train_s,
                                           batch_size = batch_size),
                              steps_per_epoch = np.ceil(1. * X_train_s.shape[0] / batch_size),
                              epochs = 360,
                              verbose = 1, validation_data = (X_test, Y_test),
                              callbacks = [optimizer_schedule])

    return(format(100 * (1 - fit.history['val_acc'][-1]), '.2f'))
  #+end_src

  #+RESULTS[7a2ce1ac6464f24c197bc9cb4cb60963b9a7d874]: wide_resnet_less_dropout_cifar100_bench
  : 28.83

  #+name:wide_resnet_cifar100_bench_no_stoch_reg
  #+begin_src python :exports none :noweb yes :cache yes :results value
    import numpy as np
    np.random.seed(42)  # for reproducibility
    from keras.layers import Dense, Flatten
    from keras.layers.convolutional import Conv2D
    from keras.layers.pooling import AveragePooling2D
    from keras.utils import np_utils
    from keras.optimizers import SGD
    from keras.preprocessing.image import ImageDataGenerator
    from keras.layers.normalization import BatchNormalization
    from keras.preprocessing.image import ImageDataGenerator
    from keras.models import Model
    from keras.layers import Input
    from keras.layers.merge import add, concatenate
    from keras import regularizers
    from keras import backend as K
    from keras.layers import Lambda
    from keras.datasets import cifar100
    from keras.callbacks import LearningRateScheduler

    <<channels_first_hb_conv>>

    def schedule(x):
        x = np.array(x, dtype = 'float32')
        lr = np.piecewise(x,[x <= 60,
                             (x > 60) & (x <= 120),
                             (x > 120) & (x <= 180),
                             (x > 180) & (x <= 240),
                             (x > 240) & (x <= 300),
                             x > 300],
                          [0.1,
                           0.1 * 0.2,
                           0.1 * 0.2 ** 2,
                           0.1 * 0.2 ** 3,
                           0.1 * 0.2 ** 4,
                           0.1 * 0.2 ** 5])
        return(float(lr))

    optimizer_schedule = LearningRateScheduler(schedule)
    batch_size = 128
    nb_epoch = 360
    hbp = 0.45
    wd = 0.0005
    nb_filters = 160
    nb_conv = 3

    (X_train_s, y_train_s), (X_test, y_test) = cifar100.load_data()
    img_rows = X_train_s.shape[1]
    img_cols = X_train_s.shape[2]
    nb_classes = np.max(y_train_s) + 1

    train_mean = np.mean(X_train_s, axis = (0, 1, 2)).reshape((1, 1, 1, 3))
    train_sd = np.std(X_train_s, axis = (0, 1, 2)).reshape((1, 1, 1, 3))

    X_train_s = (X_train_s - train_mean) / train_sd
    X_test = (X_test - train_mean) / train_sd

    # Change to channels first format for Theano
    X_train_s = np.moveaxis(X_train_s, [1, 2, 3], [2, 3, 1])
    X_test = np.moveaxis(X_test, [1, 2, 3], [2, 3, 1])

    X_train_s = X_train_s.astype('float32')
    X_test = X_test.astype('float32')

    # convert class vectors to binary class matrices
    Y_train_s = np_utils.to_categorical(y_train_s, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)

    def rectifier(x):
        return K.relu(x)

    def build_resnet_block(inp, nb_reps = 1, nb_conv = 3, nb_filters = 32,
                           hbp = 0.01, wd = 0.0001, stride = 1,
                           data_format = None):
        through = BatchNormalization(axis = 1)(inp)
        through = Lambda(rectifier)(through)
        skip = Conv2D(nb_filters, kernel_size = (1, 1), padding = 'same',
                      strides = (stride, stride), kernel_regularizer = regularizers.l2(wd),
                      use_bias = False, data_format = data_format)(through)
        through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                         padding = 'same',
                         strides = (stride, stride),
                         kernel_regularizer = regularizers.l2(wd),
                         use_bias = False, data_format = data_format)(through)
        through = BatchNormalization(axis = 1)(through)
        through = Lambda(rectifier)(through)
        through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                         padding = 'same',
                         strides = (1, 1), kernel_regularizer = regularizers.l2(wd),
                         use_bias = False, data_format = data_format)(through)
        inp = add([through, skip])
        for i in range(nb_reps):
            through = BatchNormalization(axis = 1)(inp)
            through = Lambda(rectifier)(through)
            through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'same',
                             kernel_regularizer = regularizers.l2(wd),
                             use_bias = False, data_format = data_format)(through)
            through = BatchNormalization(axis = 1)(through)
            through = Lambda(rectifier)(through)
            through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                             padding = 'same', kernel_regularizer = regularizers.l2(wd),
                             use_bias = False, data_format = data_format)(through)
            inp = add([through, inp])
        return(inp)

    data_format = "channels_first"

    inp = Input(shape=(3, img_rows, img_cols))
    exp = Conv2D(filters = 160,
                 kernel_size = (nb_conv, nb_conv),
                 padding = 'same',
                 kernel_regularizer = regularizers.l2(wd),
                 data_format = data_format,
                 use_bias = False)(inp)
    block1 = build_resnet_block(exp, nb_reps = 3, nb_conv = nb_conv,
                                nb_filters = nb_filters,
                                hbp = hbp, wd = wd, data_format = data_format)
    block2 = build_resnet_block(block1, nb_reps = 3, nb_conv = nb_conv,
                                nb_filters = 2 * nb_filters,
                                hbp = hbp, wd = wd, stride = 2,
                                data_format = data_format)
    block3 = build_resnet_block(block2, nb_reps = 3,
                                nb_conv = nb_conv, nb_filters = 4 * nb_filters,
                                hbp = hbp, wd = wd, stride = 2,
                                data_format = data_format)
    bn_pre_pool = BatchNormalization(axis = 1)(block3)
    rectifier_pre_pool = Lambda(rectifier)(bn_pre_pool)
    pool = AveragePooling2D(pool_size=(8, 8), data_format = data_format)(rectifier_pre_pool)
    flat = Flatten()(pool)
    dense = Dense(nb_classes, activation = 'softmax',
                   kernel_regularizer = regularizers.l2(wd),
                  use_bias = False)(flat)
    model = Model(inputs = inp, outputs = dense)

    datagen = ImageDataGenerator(width_shift_range = 0.15,
                                 height_shift_range = 0.15,
                                 zoom_range = 0.0,
                                 rotation_range = 0,
                                 horizontal_flip = True,
                                 fill_mode='reflect',
                                 data_format = data_format)

    sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0.00, nesterov = True)
    model.compile(loss = 'categorical_crossentropy',
                  optimizer = sgd,
                  metrics=['accuracy'])

    fit = model.fit_generator(datagen.flow(X_train_s, Y_train_s,
                                           batch_size = batch_size),
                              steps_per_epoch = np.ceil(1. * X_train_s.shape[0] / batch_size),
                              epochs = 360,
                              verbose = 1, validation_data = (X_test, Y_test),
                              callbacks = [optimizer_schedule])

    return(format(100 * (1 - fit.history['val_acc'][-1]), '.2f'))
  #+end_src

  #+RESULTS[74b919d88ea1c46c1a0cf290d0493f58d743c225]: wide_resnet_cifar100_bench_no_stoch_reg
  : 20.1

  #+name:wide_resnet_cifar10_bench_no_stoch_reg
  #+begin_src python :exports none :noweb yes :cache yes :results value
    import numpy as np
    np.random.seed(42)  # for reproducibility
    from keras.layers import Dense, Flatten
    from keras.layers.convolutional import Conv2D
    from keras.layers.pooling import AveragePooling2D
    from keras.utils import np_utils
    from keras.optimizers import SGD
    from keras.preprocessing.image import ImageDataGenerator
    from keras.layers.normalization import BatchNormalization
    from keras.preprocessing.image import ImageDataGenerator
    from keras.models import Model
    from keras.layers import Input
    from keras.layers.merge import add, concatenate
    from keras import regularizers
    from keras import backend as K
    from keras.layers import Lambda
    from keras.datasets import cifar10
    from keras.callbacks import LearningRateScheduler

    <<channels_first_hb_conv>>

    def schedule(x):
        x = np.array(x, dtype = 'float32')
        lr = np.piecewise(x,[x <= 60,
                             (x > 60) & (x <= 120),
                             (x > 120) & (x <= 180),
                             (x > 180) & (x <= 240),
                             (x > 240) & (x <= 300),
                             x > 300],
                          [0.1,
                           0.1 * 0.2,
                           0.1 * 0.2 ** 2,
                           0.1 * 0.2 ** 3,
                           0.1 * 0.2 ** 4,
                           0.1 * 0.2 ** 5])
        return(float(lr))

    optimizer_schedule = LearningRateScheduler(schedule)
    batch_size = 128
    nb_epoch = 360
    hbp = 0.45
    wd = 0.0005
    nb_filters = 160
    nb_conv = 3

    (X_train_s, y_train_s), (X_test, y_test) = cifar10.load_data()
    img_rows = X_train_s.shape[1]
    img_cols = X_train_s.shape[2]
    nb_classes = np.max(y_train_s) + 1

    train_mean = np.mean(X_train_s, axis = (0, 1, 2)).reshape((1, 1, 1, 3))
    train_sd = np.std(X_train_s, axis = (0, 1, 2)).reshape((1, 1, 1, 3))

    X_train_s = (X_train_s - train_mean) / train_sd
    X_test = (X_test - train_mean) / train_sd

    # Change to channels first format for Theano
    X_train_s = np.moveaxis(X_train_s, [1, 2, 3], [2, 3, 1])
    X_test = np.moveaxis(X_test, [1, 2, 3], [2, 3, 1])

    X_train_s = X_train_s.astype('float32')
    X_test = X_test.astype('float32')

    # convert class vectors to binary class matrices
    Y_train_s = np_utils.to_categorical(y_train_s, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)

    def rectifier(x):
        return K.relu(x)

    def build_resnet_block(inp, nb_reps = 1, nb_conv = 3, nb_filters = 32,
                           hbp = 0.01, wd = 0.0001, stride = 1,
                           data_format = None):
        through = BatchNormalization(axis = 1)(inp)
        through = Lambda(rectifier)(through)
        skip = Conv2D(nb_filters, kernel_size = (1, 1), padding = 'same',
                      strides = (stride, stride), kernel_regularizer = regularizers.l2(wd),
                      use_bias = False, data_format = data_format)(through)
        through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                         padding = 'same',
                         strides = (stride, stride),
                         kernel_regularizer = regularizers.l2(wd),
                         use_bias = False, data_format = data_format)(through)
        through = BatchNormalization(axis = 1)(through)
        through = Lambda(rectifier)(through)
        through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                         padding = 'same',
                         strides = (1, 1), kernel_regularizer = regularizers.l2(wd),
                         use_bias = False, data_format = data_format)(through)
        inp = add([through, skip])
        for i in range(nb_reps):
            through = BatchNormalization(axis = 1)(inp)
            through = Lambda(rectifier)(through)
            through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'same',
                             kernel_regularizer = regularizers.l2(wd),
                             use_bias = False, data_format = data_format)(through)
            through = BatchNormalization(axis = 1)(through)
            through = Lambda(rectifier)(through)
            through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                             padding = 'same', kernel_regularizer = regularizers.l2(wd),
                             use_bias = False, data_format = data_format)(through)
            inp = add([through, inp])
        return(inp)

    data_format = "channels_first"

    inp = Input(shape=(3, img_rows, img_cols))
    exp = Conv2D(filters = 160,
                 kernel_size = (nb_conv, nb_conv),
                 padding = 'same',
                 kernel_regularizer = regularizers.l2(wd),
                 data_format = data_format,
                 use_bias = False)(inp)
    block1 = build_resnet_block(exp, nb_reps = 3, nb_conv = nb_conv,
                                nb_filters = nb_filters,
                                hbp = hbp, wd = wd, data_format = data_format)
    block2 = build_resnet_block(block1, nb_reps = 3, nb_conv = nb_conv,
                                nb_filters = 2 * nb_filters,
                                hbp = hbp, wd = wd, stride = 2,
                                data_format = data_format)
    block3 = build_resnet_block(block2, nb_reps = 3,
                                nb_conv = nb_conv, nb_filters = 4 * nb_filters,
                                hbp = hbp, wd = wd, stride = 2,
                                data_format = data_format)
    bn_pre_pool = BatchNormalization(axis = 1)(block3)
    rectifier_pre_pool = Lambda(rectifier)(bn_pre_pool)
    pool = AveragePooling2D(pool_size=(8, 8), data_format = data_format)(rectifier_pre_pool)
    flat = Flatten()(pool)
    dense = Dense(nb_classes, activation = 'softmax',
                   kernel_regularizer = regularizers.l2(wd),
                  use_bias = False)(flat)
    model = Model(inputs = inp, outputs = dense)

    datagen = ImageDataGenerator(width_shift_range = 0.15,
                                 height_shift_range = 0.15,
                                 zoom_range = 0.0,
                                 rotation_range = 0,
                                 horizontal_flip = True,
                                 fill_mode='reflect',
                                 data_format = data_format)

    sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0.00, nesterov = True)
    model.compile(loss = 'categorical_crossentropy',
                  optimizer = sgd,
                  metrics=['accuracy'])

    fit = model.fit_generator(datagen.flow(X_train_s, Y_train_s,
                                           batch_size = batch_size),
                              steps_per_epoch = np.ceil(1. * X_train_s.shape[0] / batch_size),
                              epochs = 360,
                              verbose = 1, validation_data = (X_test, Y_test),
                              callbacks = [optimizer_schedule])

    return(format(100 * (1 - fit.history['val_acc'][-1]), '.2f'))
  #+end_src

  #+RESULTS[0b3f3a1fd63d0993e0ede3772bb243e5496adbde]: wide_resnet_cifar10_bench_no_stoch_reg
  : 4.13

  #+name:wide_resnet_mnist_bench_no_stoch_reg
  #+begin_src python :exports none :noweb yes :cache yes :results value
    import numpy as np
    np.random.seed(42)  # for reproducibility
    from keras.layers import Dense, Flatten
    from keras.layers.convolutional import Conv2D
    from keras.layers.pooling import AveragePooling2D
    from keras.utils import np_utils
    from keras.optimizers import SGD
    from keras.preprocessing.image import ImageDataGenerator
    from keras.layers.normalization import BatchNormalization
    from keras.preprocessing.image import ImageDataGenerator
    from keras.models import Model
    from keras.layers import Input
    from keras.layers.merge import add, concatenate
    from keras import regularizers
    from keras import backend as K
    from keras.layers import Lambda
    from keras.datasets import mnist
    from keras.callbacks import LearningRateScheduler

    <<channels_first_hb_conv>>

    def schedule(x):
               x = np.array(x, dtype = 'float32')
               lr = np.piecewise(x,
                                 [x <= 60,
                                  (x > 60) & (x <= 120),
                                  (x > 120) & (x <= 180),
                                  (x > 180) & (x <= 240),
                                  (x > 240) & (x <= 300),
                                  x > 300],
                                 [0.1,
                                  0.1 * 0.2,
                                  0.1 * 0.2 ** 2,
                                  0.1 * 0.2 ** 3,
                                  0.1 * 0.2 ** 4,
                                  0.1 * 0.2 ** 5])
               return(float(lr))
    optimizer_schedule = LearningRateScheduler(schedule)
    batch_size = 128
    nb_epoch = 360
    hbp = 0.45
    wd = 0.0005
    nb_filters = 160
    nb_conv = 3

    (X_train_s, y_train_s), (X_test, y_test) = mnist.load_data()
    img_rows = X_train_s.shape[1]
    img_cols = X_train_s.shape[2]

    X_train_s = X_train_s.reshape(X_train_s.shape[0], img_rows, img_cols, 1)
    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)

    nb_classes = np.max(y_train_s) + 1

    train_mean = np.mean(X_train_s, axis = (0, 1, 2)).reshape((1, 1, 1, 1))
    train_sd = np.std(X_train_s, axis = (0, 1, 2)).reshape((1, 1, 1, 1))

    X_train_s = (X_train_s - train_mean) / train_sd
    X_test = (X_test - train_mean) / train_sd

    # Change to channels first format for Theano
    X_train_s = np.moveaxis(X_train_s, [1, 2, 3], [2, 3, 1])
    X_test = np.moveaxis(X_test, [1, 2, 3], [2, 3, 1])

    X_train_s = X_train_s.astype('float32')
    X_test = X_test.astype('float32')

    # convert class vectors to binary class matrices
    Y_train_s = np_utils.to_categorical(y_train_s, nb_classes)
    Y_test = np_utils.to_categorical(y_test, nb_classes)

    def rectifier(x):
        return K.relu(x)

    def build_resnet_block(inp, nb_reps = 1, nb_conv = 3, nb_filters = 32,
                           hbp = 0.01, wd = 0.0001, stride = 1,
                           data_format = None):
        through = BatchNormalization(axis = 1)(inp)
        through = Lambda(rectifier)(through)
        skip = Conv2D(nb_filters, kernel_size = (1, 1), padding = 'same',
                      strides = (stride, stride), kernel_regularizer = regularizers.l2(wd),
                      use_bias = False, data_format = data_format)(through)
        through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                         padding = 'same',
                         strides = (stride, stride),
                         kernel_regularizer = regularizers.l2(wd),
                         use_bias = False, data_format = data_format)(through)
        through = BatchNormalization(axis = 1)(through)
        through = Lambda(rectifier)(through)
        through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                         padding = 'same',
                         strides = (1, 1), kernel_regularizer = regularizers.l2(wd),
                         use_bias = False, data_format = data_format)(through)
        inp = add([through, skip])
        for i in range(nb_reps):
            through = BatchNormalization(axis = 1)(inp)
            through = Lambda(rectifier)(through)
            through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv), padding = 'same',
                             kernel_regularizer = regularizers.l2(wd),
                             use_bias = False, data_format = data_format)(through)
            through = BatchNormalization(axis = 1)(through)
            through = Lambda(rectifier)(through)
            through = Conv2D(nb_filters, kernel_size = (nb_conv, nb_conv),
                             padding = 'same', kernel_regularizer = regularizers.l2(wd),
                             use_bias = False, data_format = data_format)(through)
            inp = add([through, inp])
        return(inp)

    data_format = "channels_first"

    inp = Input(shape=(1, img_rows, img_cols))
    exp = Conv2D(filters = 160,
                 kernel_size = (nb_conv, nb_conv),
                 padding = 'same',
                 kernel_regularizer = regularizers.l2(wd),
                 data_format = data_format,
                 use_bias = False)(inp)
    block1 = build_resnet_block(exp, nb_reps = 3, nb_conv = nb_conv,
                                nb_filters = nb_filters,
                                hbp = hbp, wd = wd, data_format = data_format)
    block2 = build_resnet_block(block1, nb_reps = 3, nb_conv = nb_conv,
                                nb_filters = 2 * nb_filters,
                                hbp = hbp, wd = wd, stride = 2,
                                data_format = data_format)
    block3 = build_resnet_block(block2, nb_reps = 3,
                                nb_conv = nb_conv, nb_filters = 4 * nb_filters,
                                hbp = hbp, wd = wd, stride = 2,
                                data_format = data_format)
    bn_pre_pool = BatchNormalization(axis = 1)(block3)
    rectifier_pre_pool = Lambda(rectifier)(bn_pre_pool)
    pool = AveragePooling2D(pool_size=(7, 7), data_format = data_format)(rectifier_pre_pool)
    flat = Flatten()(pool)
    dense = Dense(nb_classes, activation = 'softmax',
                   kernel_regularizer = regularizers.l2(wd),
                  use_bias = False)(flat)
    model = Model(inputs = inp, outputs = dense)

    sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0.00, nesterov = True)
    model.compile(loss = 'categorical_crossentropy',
                  optimizer = sgd,
                  metrics=['accuracy'])

    fit = model.fit(X_train_s, Y_train_s,
                    batch_size = batch_size,
                    epochs = 360,
                    verbose = 1, validation_data = (X_test, Y_test),
                    callbacks = [optimizer_schedule])

    return(format(100 * (1 - fit.history['val_acc'][-1]), '.2f'))
  #+end_src

  #+RESULTS[38ce3805a006ea626b82eec1f4cb28ef111ce4d6]: wide_resnet_mnist_bench_no_stoch_reg
  : 0.66


  #+name: insert
  #+begin_src python :var x="" :exports none
    return(x)
  #+end_src

  #+caption: Benchmark results.  Values are misclassification percentages.
  #+caption: The CIFAR datasets are augmented with translations and flips.  The MNIST
  #+caption: digits are not augmented.
  #+caption: @@latex:\\@@   
  #+attr_latex: :align c|cccc
  #+name: benchmark_table
  | Dataset  |   Hybrid Bootstrap | No Stochastic Reg. | Dropout                  | No Stochastic Reg.       |
  |          | (Our Architecture) | (Our Architecture) | (gls:wrn 28-10)          | (gls:wrn 28-10)          |
  | /        |<                   |                    |                          |   >                      |
  |----------+--------------------+--------------------+--------------------------+--------------------------|
  | CIFAR10  |                3.4 |               4.13 | 3.89                     | 4.00                     |
  | CIFAR100 |              18.36 |               20.1 | 18.85                    | 19.25                    |
  | MNIST    |                0.3 |               0.66 | NA                       | NA                       |
  #+tblfm: @4$2 = '(org-sbe "insert" (x "wide_resnet_cifar10_bench")))
  #+tblfm: @5$2 = '(org-sbe "insert" (x "wide_resnet_cifar100_bench")))  
  #+tblfm: @6$2 = '(org-sbe "insert" (x "wide_resnet_mnist_bench")))
  #+tblfm: @4$3 = '(org-sbe "insert" (x "wide_resnet_cifar10_bench_no_stoch_reg")))
  #+tblfm: @5$3 = '(org-sbe "insert" (x "wide_resnet_cifar100_bench_no_stoch_reg")))  
  #+tblfm: @6$3 = '(org-sbe "insert" (x "wide_resnet_mnist_bench_no_stoch_reg")))

* Other Algorithms
  <<other_algorithms>> The hybrid bootstrap is not only useful for
  glspl:cnn.  It is also applicable to other inferential algorithms
  and can be applied without modifying their underlying code by
  expanding the training set in the manner of traditional data
  augmentation.
** Multilayer Perceptron
   #+name: mlp_hb
   #+begin_src python :exports none :noweb yes :cache yes :results value
     import numpy as np
     np.random.seed(42)  # for reproducibility
     from keras.datasets import mnist
     from keras.models import Sequential
     from keras.layers import Dense, Dropout, Flatten
     from keras.utils import np_utils
     from keras.optimizers import SGD
     from keras import regularizers
     from keras.callbacks import LearningRateScheduler

     <<basic_hb>>

     def schedule(x):
           x = np.array(x, dtype = 'float32')
           lr = np.piecewise(x,
                             [x <= 250,
                              (x > 250) & (x <= 500),
                              (x > 500) & (x <= 750),
                              x > 750],
                             [0.1,
                              0.1 * 0.2,
                              0.1 * 0.2 ** 2,
                              0.1 * 0.2 ** 3])
           return(float(lr))

     optimizer_schedule = LearningRateScheduler(schedule)

     batch_size = 512
     nb_classes = 10
     wd = 0.00001

     # input image dimensions
     img_rows, img_cols = 28, 28

     # the data, shuffled and split between train and test sets
     (X_train_s, y_train_s), (X_test, y_test) = mnist.load_data()

     X_train_s = X_train_s.astype('float32')
     X_test = X_test.astype('float32')
     X_train_s /= 255
     X_test /= 255

     # convert class vectors to binary class matrices
     Y_train_s = np_utils.to_categorical(y_train_s, nb_classes)
     Y_test = np_utils.to_categorical(y_test, nb_classes)

     model = Sequential()
     model.add(Flatten(input_shape = (img_rows, img_cols)))
     model.add(HB(0.225, shift = -1))
     model.add(Dense(8192, activation = 'relu', kernel_regularizer = regularizers.l2(wd)))
     model.add(HB(0.45, shift = -2))
     model.add(Dense(8192, activation = 'relu', kernel_regularizer = regularizers.l2(wd)))
     model.add(Dense(nb_classes, activation = 'softmax',
                     kernel_regularizer = regularizers.l2(wd)))
     sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0.00, nesterov = False)
     model.compile(loss = 'categorical_crossentropy',
                   optimizer = sgd,
                   metrics = ['accuracy'])
     fit = model.fit(X_train_s, Y_train_s, batch_size = batch_size, epochs = 1000,
                     verbose = 1, validation_data = (X_test, Y_test),
                     callbacks = [optimizer_schedule])

     return(format(100 * (1 - fit.history['val_acc'][-1]), '.2f'))
   #+end_src

   #+RESULTS[9b50646efc11a2c3a2b9e3763b0b965c0afaf051]: mlp_hb
   : 0.81

   #+name: mlp_drop
   #+begin_src python :exports none :noweb yes :cache yes :results value
     import numpy as np
     np.random.seed(42)  # for reproducibility
     from keras.datasets import mnist
     from keras.models import Sequential
     from keras.layers import Dense, Dropout, Flatten
     from keras.utils import np_utils
     from keras.optimizers import SGD
     from keras import regularizers
     from keras.callbacks import LearningRateScheduler

     def schedule(x):
           x = np.array(x, dtype = 'float32')
           lr = np.piecewise(x,
                             [x <= 250,
                              (x > 250) & (x <= 500),
                              (x > 500) & (x <= 750),
                              x > 750],
                             [0.1,
                              0.1 * 0.2,
                              0.1 * 0.2 ** 2,
                              0.1 * 0.2 ** 3])
           return(float(lr))

     optimizer_schedule = LearningRateScheduler(schedule)

     batch_size = 512
     nb_classes = 10
     wd = 0.00001

     # input image dimensions
     img_rows, img_cols = 28, 28

     # the data, shuffled and split between train and test sets
     (X_train_s, y_train_s), (X_test, y_test) = mnist.load_data()

     X_train_s = X_train_s.astype('float32')
     X_test = X_test.astype('float32')
     X_train_s /= 255
     X_test /= 255

     # convert class vectors to binary class matrices
     Y_train_s = np_utils.to_categorical(y_train_s, nb_classes)
     Y_test = np_utils.to_categorical(y_test, nb_classes)

     model = Sequential()
     model.add(Flatten(input_shape = (img_rows, img_cols)))
     model.add(Dropout(0.25))
     model.add(Dense(8192, activation = 'relu', kernel_regularizer = regularizers.l2(wd)))
     model.add(Dropout(0.5))
     model.add(Dense(8192, activation = 'relu', kernel_regularizer = regularizers.l2(wd)))
     model.add(Dense(nb_classes, activation = 'softmax',
                     kernel_regularizer = regularizers.l2(wd)))
     sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0.00, nesterov = False)
     model.compile(loss = 'categorical_crossentropy',
                   optimizer = sgd,
                   metrics = ['accuracy'])
     fit = model.fit(X_train_s, Y_train_s, batch_size = batch_size, epochs = 1000,
                     verbose = 1, validation_data = (X_test, Y_test),
                     callbacks = [optimizer_schedule])

     return(format(100 * (1 - fit.history['val_acc'][-1]), '.2f'))
   #+end_src

   #+RESULTS[18d1d31eefa9c9efb3195006a0f9fec445ec9c6c]: mlp_drop
   : 1.06


   The multilayer perceptron is not of tremendous modern interest for
   image classification, but it is still an effective model for other
   tasks.  Dropout is commonly used to regularize the multilayer
   perceptron, but the hybrid bootstrap is even more effective.  As an
   example, we train a multilayer perceptron on the MNIST digits with
   2 ``hidden'' layers of $2^{13}$ neurons each with gls:relu
   activations and $u = 0.225$, $u = 0.45$ hybrid bootstrap
   regularization for each layer respectively.  We use weight decay
   0.00001 and gls:sgd with momentum 0.9 and batch size 512.  We start
   the learning rate at 0.1 and multiply it by 0.2 every 250 epochs
   for 1,000 epochs total training.  The resulting network has error
   src_python[:exports results :var x = mlp_hb :results raw]{return(x)}\%
   on the MNIST test set.  Srivastava et al. used
   dropout on the same architecture with resulting error 0.95\%
   cite:srivastava2014dropout.  However, their training schedule was
   different and they used a max-norm constraint rather than weight
   decay.  To verify that this improvement is not simply a consequence
   of these differences rather than the result of replacing dropout
   with the hybrid bootstrap, we use the same parameters but replace
   the hybrid bootstrap with dropout $p = 0.25$, $p = 0.5$
   respectively.  The resulting network has test set error
   src_python[:exports results :var x = mlp_drop :results raw]{return(x)}\%.

** Boosted Trees
   #+name: accuracy_vs_expansion_generator
   #+begin_src python :exports none :noweb yes :cache yes
     from keras.datasets import mnist
     import xgboost as xgb
     import numpy as np
     np.random.seed(42)
     import pandas as pd
     # xgboost parameters
     param = {'max_depth':6, 'eta':1., 'silent':1, 'objective':'multi:softmax', 'lambda':0.0001}
     param['nthread'] = 6
     param['eval_metric'] = ['merror', 'mlogloss']
     param['num_class'] = 10
     num_round = 500

     (X_train_s, y_train_s), (X_test, y_test) = mnist.load_data()

     # Not strictly necessary for trees.
     X_train_s = np.reshape(X_train_s, (X_train_s.shape[0], -1))
     X_test = np.reshape(X_test, (X_test.shape[0], -1))
     X_train_s = X_train_s.astype('float32')
     X_test = X_test.astype('float32')
     X_train_s /= 255
     X_test /= 255
     nb_classes = 10

     # Select the first 100 of each class
     small_indices = np.array([], dtype = 'int32')
     print('here0')
     for i in range(nb_classes):
         first_100_i_indices = np.arange(X_train_s.shape[0])[y_train_s == i][0:100]
         small_indices = np.concatenate([small_indices, first_100_i_indices])

     small_X_train = X_train_s[small_indices]
     small_y_train = y_train_s[small_indices]

     data = pd.DataFrame(columns = ['hbp',
                                    'expansion_factor',
                                    'error',
                                    'method'])
     for method in ['hybrid bootstrap', 'dropout']:
         for expansion_factor in np.exp2(range(11)).astype('int32'):        
             if method == 'hybrid bootstrap':
                 hbp = 0.45

             if method == 'dropout':
                 hbp = 0.65

             batch_size = small_X_train.shape[0]
             augmented_n = expansion_factor * small_X_train.shape[0]
             X_train = np.zeros((augmented_n, np.product(X_train_s.shape[1:])))
             bootstrap_indices = np.random.randint(small_X_train.shape[0], size = augmented_n)
             probs = 1 - np.random.uniform(0, hbp, augmented_n)
             probs = np.expand_dims(probs, axis = 0).T
             
             # Fill up the expanded training data in batches
             for i in np.arange(expansion_factor):
                 batch_indices = np.arange(i * batch_size, ((i + 1) * batch_size))
                 mask = np.random.binomial(n = 1, p = probs[batch_indices], size = (batch_size, small_X_train.shape[1]))
                 if method == 'hybrid bootstrap':
                     X_train[batch_indices] = small_X_train * mask  + small_X_train[bootstrap_indices[batch_indices]] * (1 - mask)
                 if method == 'dropout':
                     X_train[batch_indices] = small_X_train * mask * 1 / probs[batch_indices]
                     
             D_train = xgb.DMatrix(X_train, label = np.tile(small_y_train, expansion_factor))
             D_test = xgb.DMatrix(X_test, y_test)

             # Define the evaluation metrics
             evallist  = [(D_test,'eval'), (D_train,'train')]
             bst = xgb.train(param, D_train, num_round, evallist)

             error = 1 - 1. * np.sum(bst.predict(xgb.DMatrix(X_test)) == y_test) / y_test.shape[0]
             data = data.append({'hbp':hbp,
                                 'expansion_factor':expansion_factor,
                                 'error':error,
                                 'method':method}, ignore_index = True)



     param['eta'] = 0.01
     bst_baseline = xgb.train(param,
                              xgb.DMatrix(small_X_train, label = small_y_train),
                              2000,
                              evallist)
     error = 1 - 1. * np.sum(bst_baseline.predict(xgb.DMatrix(X_test)) == y_test) / y_test.shape[0]
     data = data.append({'hbp':-1,
                         'expansion_factor':1,
                         'error':error,
                         'method':'baseline'}, ignore_index = True)
     data.to_csv('./data/tree_data.csv')

   #+end_src
   #+RESULTS[128d6dfea7fc8ae7036ed96708d1a4eea0249af6]: accuracy_vs_expansion_generator
   : None

   One of the most effective classes of prediction algorithms is that
   based on gradient boosted trees described by Friedman
   cite:friedman2001greedy. Boosted tree algorithms are not very
   competitive with glspl:cnn on image classification problems, but
   they are remarkably effective for prediction problems in general
   and have the same need for regularization as other nonparametric
   models.  We use XGBoost cite:chen2016xgboost, a popular
   implementation of gradient boosted trees.  

   Vinayak and Gilad-Bachrach proposed dropping the constituent models
   of the booster during training, similar to dropout
   cite:pmlr-v38-korlakaivinayak15.  This requires modifying the
   underlying model fitting, which we have not attempted with the
   hybrid bootstrap.  However, if we naively generate hybrid bootstrap
   data on 1,000 MNIST digits with the hyperparameters $u = 0.45$ and
   $u = 0.65$ for the hybrid bootstrap and dropout respectively, we
   can see that the hybrid bootstrap outperforms dropout in Figure
   [[mnist_trees]].  We note that extreme expansion of the training data
   by hybrid bootstrap sampling seems to be important for peak
   predictive performance.  However, this must be balanced by
   consideration of the computational cost.
   #+name: mnist_expansion_plot
   #+begin_src R :exports results :file mnist_trees.pdf
     library(dplyr)
     library(ggplot2)
     library(gridExtra)
     library(tidyr)
     data <- read.csv("./data/tree_data.csv")
     data$method <- factor(data$method, levels = c("hybrid bootstrap", "dropout", "baseline"),
                           labels = c("Hybrid Bootstrap", "Dropout", "Baseline"))

     acc_plot <- ggplot(data = data[data$method != "Baseline",], aes(x = expansion_factor, y = error, color = method)) +
         geom_line() +
         geom_point() + 
         geom_hline(aes(yintercept = data[data$method == "Baseline",][,"error"])) + 
         scale_x_continuous(trans = "log2", breaks = 2 ^ (0:10), minor_breaks = NULL) + 
         scale_y_continuous(trans = "log2", minor_breaks = seq(0, 1, 0.01), breaks = seq(0.05, 0.2, 0.02)) +
         xlab("Expansion Multiple") + 
         ylab("Test Set Misclassification Rate") +
         guides(color = guide_legend(title = "Method"))


     ggsave("mnist_trees.pdf",
            acc_plot,
            device = "pdf",
            width = 6.5,
            height = 4,
            units = "in")
  #+end_src
   #+caption: Comparison of boosted tree test set performance on the MNIST digits for stochastic expansions of 1,000 training images.
   #+caption:  The horizontal line is the performance of an XGBoost model using four times as many trees and a smaller step size,
   #+caption:  but no additional regularization.
   #+label: mnist_trees
   #+RESULTS: mnist_expansion_plot
   [[file:mnist_trees.pdf]]
   
   #+name:xgboost_cancer_generator
  #+begin_src python :exports none :noweb yes :cache yes
    import xgboost as xgb
    import numpy as np
    np.random.seed(42)
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import log_loss
    from sklearn.model_selection import KFold

    nb_cross_val_splits = 5
    kf = KFold(n_splits = nb_cross_val_splits, shuffle = True, random_state = 42)

    data = np.array(pd.read_csv("./data/breast_cancer.csv"))
    data[:, 1] = 1 - pd.factorize(data[:, 1])[0]
    data = data[:, 1:32]


    X = data[:, 1:]
    X = X.astype('float32')

    # Malignant is 1
    y = data[:, 0]
    y = y.astype('int32')
    small_X_train, X_test, small_y_train, y_test = train_test_split(X, y,
                                                                    test_size = 0.5,
                                                                    random_state = 42)

    # xgboost parameters
    param = {}
    param['max_depth'] = 2
    param['eta'] = 0.01
    param['silent'] = 1
    param['objective'] = 'binary:logistic'
    param['nthread'] = 6
    param['eval_metric'] = ['error', 'logloss']
    num_round = 50000

    data_frame = pd.DataFrame(columns = ['hbp',
                                         'expansion_factor',
                                         'logloss',
                                         'lambda',
                                         'method',
                                         'best_ntree',
                                         'error'])

    method = 'basic'
    hbp = 0
    expansion_factor = 1
    for l in np.exp2(np.arange(-10, 10)):
        best_ntree_folds = np.zeros((nb_cross_val_splits, ))
        folds = kf.split(small_X_train)
        for fold, fold_number in zip(folds, np.arange(nb_cross_val_splits)):
            train_indices = fold[0]
            valid_indices = fold[1]
            X_train = small_X_train[train_indices]
            y_train = np.tile(small_y_train[train_indices], expansion_factor).flatten()
            D_train = xgb.DMatrix(X_train, label = y_train)
            D_valid = xgb.DMatrix(small_X_train[valid_indices], small_y_train[valid_indices])
            evallist  = [(D_valid,'eval'), (D_train,'train')]
            store_trace = {}
            param['lambda'] = l
            bst = xgb.train(param, D_train, num_round, evallist, evals_result = store_trace)
            best_logloss = np.min(store_trace['eval']['logloss'])
            is_best = store_trace['eval']['logloss'] == best_logloss
            best_ntree_folds[fold_number] = np.arange(store_trace['eval']['logloss'].__len__())[is_best][0]
        average_best_ntree = int(np.median(best_ntree_folds))
        X_train = small_X_train
        y_train = np.tile(small_y_train, expansion_factor).flatten()
        D_train = xgb.DMatrix(X_train, label = y_train)
        D_test = xgb.DMatrix(X_test, y_test)
        evallist  = [(D_test,'eval'), (D_train,'train')]
        store_trace = {}
        param['lambda'] = l
        bst = xgb.train(param, D_train, num_round, evallist, evals_result = store_trace)
        logloss = store_trace['eval']['logloss'][average_best_ntree]
        error = store_trace['eval']['error'][average_best_ntree]
        data_frame = data_frame.append({'hbp':hbp,
                                       'expansion_factor':expansion_factor,
                                       'logloss':logloss,
                                       'lambda':l,
                                       'method':method,
                                       'best_ntree':average_best_ntree,
                                       'error':error}, ignore_index = True)

    l = 0.0001
    param['lambda'] = l
    expansion_factor = 1000
    num_round = 3000
    for method in ['hybrid_bootstrap', 'dropout',
                   'hybrid_bootstrap_normalized',
                   'hybrid_bootstrap_permuted_normalization',
                   'just_scale',
                   'dropout_no_norm']:
        for hbp in np.exp2(np.arange(-1, 5, 0.25)) * 0.01:
            best_ntree_folds = np.zeros((nb_cross_val_splits, ))
            folds = kf.split(small_X_train)
            for fold, fold_number in zip(folds, np.arange(nb_cross_val_splits)):
                batch_size = fold[0].shape[0]
                train_indices = fold[0]
                valid_indices = fold[1]
                augmented_n = expansion_factor * train_indices.shape[0]
                X_train = np.zeros((augmented_n, np.product(small_X_train.shape[1:])))
                bootstrap_indices = np.random.choice(train_indices, size = augmented_n)
                probs = 1 - np.random.uniform(0, hbp, augmented_n)
                probs = np.expand_dims(probs, axis = 0).T
                for i in range(expansion_factor):
                    batch_indices = np.arange(i * batch_size,((i + 1) * batch_size))
                    mask = np.random.binomial(n = 1, p = probs[batch_indices], size = (batch_size, small_X_train.shape[1]))
                    if method == 'hybrid_bootstrap':
                        X_train[batch_indices] = small_X_train[train_indices] * mask + small_X_train[bootstrap_indices[batch_indices]] * (1 - mask)
                    if method == 'dropout':
                        X_train[batch_indices] = small_X_train[train_indices] * mask / probs[batch_indices]
                    if method == 'dropout_no_norm':
                        X_train[batch_indices] = small_X_train[train_indices] * mask 
                    if method == 'hybrid_bootstrap_normalized':
                        X_train[batch_indices] = small_X_train[train_indices] * mask / probs[batch_indices] + small_X_train[bootstrap_indices[batch_indices]] * (1 - mask)
                    if method == 'hybrid_bootstrap_permuted_normalization':
                        X_train[batch_indices] = small_X_train[train_indices] * mask / probs[np.random.permutation(batch_indices)] + small_X_train[bootstrap_indices[batch_indices]] * (1 - mask)
                    if method == 'just_scale':
                        X_train[batch_indices] = small_X_train[train_indices] / probs[batch_indices]
                y_train = np.tile(small_y_train[train_indices], expansion_factor).flatten()
                D_train = xgb.DMatrix(X_train, label = y_train)
                D_valid = xgb.DMatrix(small_X_train[valid_indices], small_y_train[valid_indices])
                evallist  = [(D_valid,'eval'), (D_train,'train')]
                store_trace = {}
                param['lambda'] = l
                bst = xgb.train(param, D_train, num_round, evallist, evals_result = store_trace)
                best_logloss = np.min(store_trace['eval']['logloss'])
                is_best = store_trace['eval']['logloss'] == best_logloss
                best_ntree_folds[fold_number] = np.arange(store_trace['eval']['logloss'].__len__())[is_best][0]
            average_best_ntree = int(np.median(best_ntree_folds))
            batch_size = small_X_train.shape[0]
            augmented_n = expansion_factor * small_X_train.shape[0]
            X_train = np.zeros((augmented_n, np.product(small_X_train.shape[1:])))
            bootstrap_indices = np.random.choice(small_X_train.shape[0], size = augmented_n)
            probs = 1 - np.random.uniform(0, hbp, augmented_n)
            probs = np.expand_dims(probs, axis = 0).T
            for i in range(expansion_factor):
                batch_indices = np.arange(i * batch_size,((i + 1) * batch_size))
                mask = np.random.binomial(n = 1, p = probs[batch_indices], size = (batch_size, small_X_train.shape[1]))
                if method == 'hybrid_bootstrap':
                    X_train[batch_indices] = small_X_train * mask  + small_X_train[bootstrap_indices[batch_indices]] * (1 - mask)
                if method == 'dropout':
                    X_train[batch_indices] = small_X_train * mask / probs[batch_indices]
                if method == 'dropout_no_norm':
                    X_train[batch_indices] = small_X_train * mask
                if method == 'hybrid_bootstrap_normalized':
                    X_train[batch_indices] = small_X_train * mask / probs[batch_indices]  + small_X_train[bootstrap_indices[batch_indices]] * (1 - mask)
                if method == 'hybrid_bootstrap_permuted_normalization':
                    X_train[batch_indices] = small_X_train * mask / probs[np.random.permutation(batch_indices)]  + small_X_train[bootstrap_indices[batch_indices]] * (1 - mask)
                if method == 'just_scale':
                        X_train[batch_indices] = small_X_train / probs[batch_indices]
            y_train = np.tile(small_y_train, expansion_factor).flatten()
            D_train = xgb.DMatrix(X_train, label = y_train)
            D_test = xgb.DMatrix(X_test, y_test)
            evallist  = [(D_test,'eval'), (D_train,'train')]
            store_trace = {}
            bst = xgb.train(param, D_train, num_round, evallist, evals_result = store_trace)
            logloss = store_trace['eval']['logloss'][average_best_ntree]
            error = store_trace['eval']['error'][average_best_ntree]
            data_frame = data_frame.append({'hbp':hbp,
                                           'expansion_factor':expansion_factor,
                                           'logloss':logloss,
                                           'lambda':l,
                                           'method':method,
                                           'best_ntree':average_best_ntree,
                                           'error':error}, ignore_index = True)

    data_frame.to_csv('./data/cancer_regularizations.csv')
  #+end_src 
   
  #+RESULTS[82484748dba8f5baa13fd4e8e3f5e23c05fa2409]: xgboost_cancer_generator
  : None

   #+name: titanic_data_prep
   #+begin_src R :exports none :cache yes
     # Data info http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3info.txt
     # Data location for test set answers biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls
     library(titanic)
     library(gdata)

     prep_data <- function(data){
       X <- data.frame(data$Pclass,
                  as.numeric(factor(data$Sex, levels = c("female", "male"))),
                  data$Age,
                  data$SibSp,
                  data$Parch,
                  data$Fare,
                  as.numeric(factor(data$Embarked, levels = c("C", "Q", "S"))))
       colnames(X) <- c("Class",
                        "Sex",
                        "Age",
                        "Siblings_and_Spouses",
                        "Parents_and_Children",
                        "Fare",
                        "Embarkation_Location")
       X[is.na(X)] <- -1
       return(list(X = X, y = data$Survived))
     }

     test_full <- read.xls("./data/titanic3.xls", sheet = 1, header = TRUE)
     test_full$name <- as.character(test_full$name)
     test_full$name <- gsub("\\\\", "", test_full$name)
     test_full$age <- round(test_full$age, 2)
     titanic_test$name <- gsub("\\\\", "", titanic_test$Name)
     titanic_test$name <- gsub("\"", "", titanic_test$name)
     merged_test <- merge(titanic_test, test_full, by.x = c("name", "Age"), by.y = c("name", "age"))
     merged_test$Survived <- merged_test$survived

     train <- prep_data(titanic_train)
     test <- prep_data(merged_test)
     train_X <- train[[1]]
     train_y <- train[[2]]
     test_X <- test[[1]]
     test_y <- test[[2]]

     write.csv(train_X, file = "./data/titanic_train_X.csv", row.names = F)
     write.csv(train_y, file = "./data/titanic_train_y.csv", row.names = F)
     write.csv(test_X, file = "./data/titanic_test_X.csv", row.names = F)
     write.csv(test_y, file = "./data/titanic_test_y.csv", row.names = F)
   #+end_src

   #+RESULTS[a93c78f32107ab2b560bfb2a11b8297ec62cf9e3]: titanic_data_prep

   #+name:xgboost_titanic_generator
   #+begin_src python :exports none :noweb yes :cache yes
     import xgboost as xgb
     import numpy as np
     np.random.seed(42)
     import pandas as pd
     from sklearn.model_selection import train_test_split
     from sklearn.metrics import log_loss
     from sklearn.model_selection import KFold

     nb_cross_val_splits = 5
     kf = KFold(n_splits = nb_cross_val_splits, shuffle = True, random_state = 42)

     small_X_train = np.array(pd.read_csv("./data/titanic_train_X.csv"))
     X_test = np.array(pd.read_csv("./data/titanic_test_X.csv"))
     small_y_train = np.array(pd.read_csv("./data/titanic_train_y.csv")).flatten()
     y_test = np.array(pd.read_csv("./data/titanic_test_y.csv")).flatten()

     # xgboost parameters
     param = {}
     param['max_depth'] = 2
     param['eta'] = 0.01
     param['silent'] = 1
     param['objective'] = 'binary:logistic'
     param['nthread'] = 6
     param['eval_metric'] = ['error', 'logloss']
     num_round = 50000

     data_frame = pd.DataFrame(columns = ['hbp',
                                          'expansion_factor',
                                          'logloss',
                                          'lambda',
                                          'method',
                                          'best_ntree',
                                          'error'])

     method = 'basic'
     hbp = 0
     expansion_factor = 1
     for l in np.exp2(np.arange(-10, 10)):
         best_ntree_folds = np.zeros((nb_cross_val_splits, ))
         folds = kf.split(small_X_train)
         for fold, fold_number in zip(folds, np.arange(nb_cross_val_splits)):
             train_indices = fold[0]
             valid_indices = fold[1]
             X_train = small_X_train[train_indices]
             y_train = np.tile(small_y_train[train_indices], expansion_factor).flatten()
             D_train = xgb.DMatrix(X_train, label = y_train)
             D_valid = xgb.DMatrix(small_X_train[valid_indices], small_y_train[valid_indices])
             evallist  = [(D_valid,'eval'), (D_train,'train')]
             store_trace = {}
             param['lambda'] = l
             bst = xgb.train(param, D_train, num_round, evallist, evals_result = store_trace)
             best_logloss = np.min(store_trace['eval']['logloss'])
             is_best = store_trace['eval']['logloss'] == best_logloss
             best_ntree_folds[fold_number] = np.arange(store_trace['eval']['logloss'].__len__())[is_best][0]
         average_best_ntree = int(np.median(best_ntree_folds))
         X_train = small_X_train
         y_train = np.tile(small_y_train, expansion_factor).flatten()
         D_train = xgb.DMatrix(X_train, label = y_train)
         D_test = xgb.DMatrix(X_test, y_test)
         evallist  = [(D_test,'eval'), (D_train,'train')]
         store_trace = {}
         param['lambda'] = l
         bst = xgb.train(param, D_train, num_round, evallist, evals_result = store_trace)
         logloss = store_trace['eval']['logloss'][average_best_ntree]
         error = store_trace['eval']['error'][average_best_ntree]
         data_frame = data_frame.append({'hbp':hbp,
                                        'expansion_factor':expansion_factor,
                                        'logloss':logloss,
                                        'lambda':l,
                                        'method':method,
                                        'best_ntree':average_best_ntree,
                                        'error':error}, ignore_index = True)

     l = 0.0001
     param['lambda'] = l
     expansion_factor = 1000
     num_round = 3000
     for method in ['hybrid_bootstrap', 'dropout',
                    'hybrid_bootstrap_normalized',
                    'hybrid_bootstrap_permuted_normalization',
                    'just_scale',
                    'dropout_no_norm']:
         for hbp in np.exp2(np.arange(-1, 5, 0.25)) * 0.01:
             best_ntree_folds = np.zeros((nb_cross_val_splits, ))
             folds = kf.split(small_X_train)
             for fold, fold_number in zip(folds, np.arange(nb_cross_val_splits)):
                 batch_size = fold[0].shape[0]
                 train_indices = fold[0]
                 valid_indices = fold[1]
                 augmented_n = expansion_factor * train_indices.shape[0]
                 X_train = np.zeros((augmented_n, np.product(small_X_train.shape[1:])))
                 bootstrap_indices = np.random.choice(train_indices, size = augmented_n)
                 probs = 1 - np.random.uniform(0, hbp, augmented_n)
                 probs = np.expand_dims(probs, axis = 0).T
                 for i in range(expansion_factor):
                     batch_indices = np.arange(i * batch_size,((i + 1) * batch_size))
                     mask = np.random.binomial(n = 1, p = probs[batch_indices], size = (batch_size, small_X_train.shape[1]))
                     if method == 'hybrid_bootstrap':
                         X_train[batch_indices] = small_X_train[train_indices] * mask + small_X_train[bootstrap_indices[batch_indices]] * (1 - mask)
                     if method == 'dropout':
                         X_train[batch_indices] = small_X_train[train_indices] * mask / probs[batch_indices]
                     if method == 'dropout_no_norm':
                         X_train[batch_indices] = small_X_train[train_indices] * mask
                     if method == 'hybrid_bootstrap_normalized':
                         X_train[batch_indices] = small_X_train[train_indices] * mask / probs[batch_indices] + small_X_train[bootstrap_indices[batch_indices]] * (1 - mask)
                     if method == 'hybrid_bootstrap_permuted_normalization':
                         X_train[batch_indices] = small_X_train[train_indices] * mask / probs[np.random.permutation(batch_indices)] + small_X_train[bootstrap_indices[batch_indices]] * (1 - mask)
                     if method == 'just_scale':
                         X_train[batch_indices] = small_X_train[train_indices] / probs[batch_indices]
                 y_train = np.tile(small_y_train[train_indices], expansion_factor).flatten()
                 D_train = xgb.DMatrix(X_train, label = y_train)
                 D_valid = xgb.DMatrix(small_X_train[valid_indices], small_y_train[valid_indices])
                 evallist  = [(D_valid,'eval'), (D_train,'train')]
                 store_trace = {}
                 param['lambda'] = l
                 bst = xgb.train(param, D_train, num_round, evallist, evals_result = store_trace)
                 best_logloss = np.min(store_trace['eval']['logloss'])
                 is_best = store_trace['eval']['logloss'] == best_logloss
                 best_ntree_folds[fold_number] = np.arange(store_trace['eval']['logloss'].__len__())[is_best][0]
             average_best_ntree = int(np.median(best_ntree_folds))
             batch_size = small_X_train.shape[0]
             augmented_n = expansion_factor * small_X_train.shape[0]
             X_train = np.zeros((augmented_n, np.product(small_X_train.shape[1:])))
             bootstrap_indices = np.random.choice(small_X_train.shape[0], size = augmented_n)
             probs = 1 - np.random.uniform(0, hbp, augmented_n)
             probs = np.expand_dims(probs, axis = 0).T
             for i in range(expansion_factor):
                 batch_indices = np.arange(i * batch_size,((i + 1) * batch_size))
                 mask = np.random.binomial(n = 1, p = probs[batch_indices], size = (batch_size, small_X_train.shape[1]))
                 if method == 'hybrid_bootstrap':
                     X_train[batch_indices] = small_X_train * mask  + small_X_train[bootstrap_indices[batch_indices]] * (1 - mask)
                 if method == 'dropout':
                     X_train[batch_indices] = small_X_train * mask / probs[batch_indices]
                 if method == 'dropout_no_norm':
                     X_train[batch_indices] = small_X_train * mask
                 if method == 'hybrid_bootstrap_normalized':
                     X_train[batch_indices] = small_X_train * mask / probs[batch_indices]  + small_X_train[bootstrap_indices[batch_indices]] * (1 - mask)
                 if method == 'hybrid_bootstrap_permuted_normalization':
                     X_train[batch_indices] = small_X_train * mask / probs[np.random.permutation(batch_indices)]  + small_X_train[bootstrap_indices[batch_indices]] * (1 - mask)
                 if method == 'just_scale':
                         X_train[batch_indices] = small_X_train / probs[batch_indices]
             y_train = np.tile(small_y_train, expansion_factor).flatten()
             D_train = xgb.DMatrix(X_train, label = y_train)
             D_test = xgb.DMatrix(X_test, y_test)
             evallist  = [(D_test,'eval'), (D_train,'train')]
             store_trace = {}
             bst = xgb.train(param, D_train, num_round, evallist, evals_result = store_trace)
             logloss = store_trace['eval']['logloss'][average_best_ntree]
             error = store_trace['eval']['error'][average_best_ntree]
             data_frame = data_frame.append({'hbp':hbp,
                                            'expansion_factor':expansion_factor,
                                            'logloss':logloss,
                                            'lambda':l,
                                            'method':method,
                                            'best_ntree':average_best_ntree,
                                            'error':error}, ignore_index = True)

     data_frame.to_csv('./data/titanic_regularizations.csv')

   #+end_src

   #+RESULTS[d81e7d084baf57d6a74b8d2429be530e5b6c18d8]: xgboost_titanic_generator
   : None

   #+name: xgboost_cancer_plot
   #+begin_src R :exports results :file xgboost_cancer.pdf
     library(ggplot2)
     library(dplyr)
     library(gridExtra)
     data <- read.csv("./data/cancer_regularizations.csv")
     data$method <- factor(data$method,
                           levels = c("basic", 
                                      "dropout",
                                      "hybrid_bootstrap",
                                      "hybrid_bootstrap_normalized",
                                      "hybrid_bootstrap_permuted_normalization",
                                      "just_scale",
                                      "dropout_no_norm"),
                           labels = c("l2",
                                      "Dropout",
                                      "Hybrid Bootstrap",
                                      "Hybrid Bootstrap\nNormalized",
                                      "Hybrid Bootstrap\nPermuted\nNormalization",
                                      "Randomly Scale",
                                      "Dropout\nUnnormalized"))

     xgboost_reg <- filter(data, method == 'l2')
     plot1 <- ggplot(xgboost_reg, aes(x = lambda, y = logloss, color = method)) +
         geom_line() +
         geom_point() +
         geom_hline(aes(yintercept = min, color = method), 
                    summarise(group_by(data, method), min = min(logloss)),
                    linetype = 2) +
         ylim(c(0.05, 0.14)) +
         xlim(c(0, 18)) +
         scale_x_continuous(trans = "log2", labels = function(x){log2(x)}) +
         ggtitle(expression(italic(l)[2]*' Reg. (Cancer Data)')) +
         guides(color = guide_legend(title = "Method")) + 
         xlab(expression('log'[2]*' '*lambda)) +
         ylab("Logloss") +
         geom_text(mapping = aes(label = best_ntree), y = 0.135,
                   show.legend = F, angle = 90, size = 2) +
         theme(legend.key.size = unit(1.8, 'lines'))

     stoch_reg <- filter(data, method != "l2")
     plot2 <- ggplot(stoch_reg, aes(x = hbp, y = logloss, color = method)) +
         geom_line() +
         geom_point() + 
         geom_hline(aes(yintercept = min, color = method), 
                    summarise(group_by(data, method), min = min(logloss)),
                    linetype = 2) +
         ylim(c(0.05, 0.14)) + 
         ggtitle("Stochastic Reg. (Cancer Data)") + 
         scale_x_continuous(trans = "log2", labels = function(x){log2(x)}) +
         guides(color = guide_legend(title = "Method")) +
         xlab(expression('log'[2]*' '*italic(u))) +
         ylab("Logloss") +
         theme(legend.position = "none") +
         geom_text(mapping = aes(label = best_ntree,
                                 y = (3 - as.numeric(method)) * 1000 + 0.135 ),
                   show.legend = F, angle = 90, size = 2)


     data <- read.csv("./data/titanic_regularizations.csv")
     data$method <- factor(data$method,
                           levels = c("basic", 
                                      "dropout",
                                      "hybrid_bootstrap",
                                      "hybrid_bootstrap_normalized",
                                      "hybrid_bootstrap_permuted_normalization",
                                      "just_scale",
                                      "dropout_no_norm"),
                           labels = c("l2",
                                      "Dropout",
                                      "Hybrid Bootstrap",
                                      "Hybrid Bootstrap\nNormalized",
                                      "Hybrid Bootstrap\nPermuted\nNormalization",
                                      "Randomly Scale",
                                      "Dropout\nUnnormalized"))
     xgboost_reg <- filter(data, method == 'l2')
     plot3 <- ggplot(xgboost_reg, aes(x = lambda, y = logloss, color = method)) +
         geom_line() +
         geom_point() +
         geom_hline(aes(yintercept = min, color = method), 
                    summarise(group_by(data, method), min = min(logloss)),
                    linetype = 2) +
         ylim(c(0.45, 1.0)) +
         xlim(c(0, 18)) +
         scale_x_continuous(trans = "log2", labels = function(x){log2(x)}) +
         ggtitle(expression(italic(l)[2]*' Reg. (Titanic Data)')) +
         guides(color = guide_legend(title = "Method")) +
         xlab(expression('log'[2]*' '*lambda)) +
         ylab("Logloss") +
         geom_text(mapping = aes(label = best_ntree),
                   y = 0.95, show.legend = F, angle = 90, size = 2) +
         theme(legend.key.size = unit(1.8, 'lines'))

     stoch_reg <- filter(data, method != "l2")
     plot4 <- ggplot(stoch_reg, aes(x = hbp, y = logloss, color = method)) +
         geom_line() +
         geom_point() + 
         geom_hline(aes(yintercept = min, color = method), 
                    summarise(group_by(data, method), min = min(logloss)),
                    linetype = 2) +
         ylim(c(0.45, 1)) + 
         ggtitle("Stochastic Reg. (Titanic Data)") + 
         scale_x_continuous(trans = "log2", labels = function(x){log2(x)}) +
         guides(color = guide_legend(title = "Method"))+
         xlab(expression('log'[2]*' '*italic(u))) +
         ylab("Logloss") +
         theme(legend.position = "none") +
         geom_text(mapping = aes(label = best_ntree,
                                 y = (3 - as.numeric(method)) * 1000 + 0.95 ),
                   show.legend = F, angle = 90, size = 2)



     plot <- grid.arrange(plot2, plot1, plot4, plot3, ncol = 2)
     ggsave("xgboost_cancer.pdf", plot,
            device = "pdf",
            width = 8.5,
            height = 6,
            units = "in")
   #+end_src
   #+caption: Comparison of different regularization mechanisms for boosted 
   #+caption: trees for breast cancer malignancy (top) and Titanic passenger
   #+caption: survival (bottom).  The number of trees selected using cross 
   #+caption: validation is printed at the top of each panel.
   #+caption: Horizontal lines are at the minimum of each curve to aid comparison,
   #+caption: but the variation of each curve is of significant importance too.  
   #+label: xgboost_cancer
   #+RESULTS: xgboost_cancer_plot


   We also compare dropout and the hybrid bootstrap for the breast
   cancer dataset where malignancy is predicted from a set of 30
   features cite:street1993nuclear.  XGBoost provides its own
   $l^2$-type regularization technique that we typically set to a
   negligible level when using the hybrid bootstrap.  We compare
   XGBoost`s $l^2$ regularization with several stochastic methods.
   The stochastic methods are: the hybrid bootstrap, dropout, the
   hybrid bootstrap with the dropout normalization, the hybrid
   bootstrap with a random permutation of the dropout normalization,
   dropout without the normalization, and just the dropout
   normalization.  We expand the training dataset by a factor of 1,000
   for the stochastic methods.  Early experiments on depth-one trees
   indicated that simply randomly scaling the data was effective but
   this does not seem to apply to other tree depths.  We randomly
   split the 569 observations into a training set and a test set and
   use the median of a five-fold cross validation to select the
   appropriate number of depth-two trees.  We allow at most 50,000
   trees for the $l^2$ regularized method and 3,000 trees for the
   stochastic regularization methods.  We perform the same the same
   procedure for the well-known Titanic survival data cite:titanic
   except the test set is chosen to be the canonical test set.  We use
   only numeric predictors and factor levels with three or fewer
   levels for the Titanic data, leaving us with $p = 7$ and 891 and
   418 observations in the training and test sets respectively.  The
   results are given in Figure [[xgboost_cancer]].  The hybrid bootstrap
   outperforms the $l^2$ regularization in both cases (very marginally
   in the case of the Titanic data).  The normalized stochastic
   methods do not seem to be terribly effective, particularly for the
   Titanic data.  We suspect this is because they replace the the
   factor data with non-factor values.  We note that training using
   the augmented data takes much longer than the $l^2$ regularization.
   However, the number of trees selected for the $l^2$ regularization
   method (printed in the same figure) may be significantly larger
   than for the stochastic methods, so the stochastic regularizers may
   offer a computational advantage at inference time.  The sizes of
   these datasets are small, and we note that the $l^2$ regularization
   has a lower (oracle) classification error for the cancer data.  Of
   course, nothing prevents one from using both regularization schemes
   as part of an ensemble, which works well in our experience.

* Discussion
  The hybrid bootstrap is an effective form of regularization.  It can
  be applied in the same fashion as the tremendously popular dropout
  technique but offers superior performance.  The hybrid bootstrap can
  easily be incorporated into other existing algorithms.  Simply
  construct hybrid bootstrap data as we do in Section
  [[other_algorithms]].  Unlike other noising schemes, the hybrid
  bootstrap does not change the support of the data.  However, the
  hybrid bootstrap does have some disadvantages.  The hybrid bootstrap
  requires the choice of at least one additional hyperparameter.  We
  have attempted to mitigate this disadvantage by sampling the hybrid
  bootstrap level, which makes performance less sensitive to the
  hyperparameter.  The hybrid bootstrap performs best when the
  original dataset is greatly expanded. The magnitude of this
  disadvantage depends on the scenario in which supervised learning is
  being used.  We think that any case where dropout is being used is a
  good opportunity to use the hybrid bootstrap.  However, there are
  some cases, such as linear regression, where the hybrid bootstrap
  seems to offer roughly the same predictive performance as existing
  methods, such as ridge regression, but at a much higher
  computational cost.  The hybrid bootstrap's performance may depend
  on the basis in which the data are presented.  This disadvantage is
  common to many algorithms.  One reason we think the hybrid bootstrap
  works so well for neural networks is that they can create features
  in a new basis at each layer that can themselves be hybrid
  bootstrapped, so the initial basis is not as important as it may be
  for other algorithms.

  We have given many examples of the hybrid bootstrap working, but
  have devoted little attention to explaining why it works.  There is
  a close relationship between hypothesis testing and regularization.
  For instance, the limiting behavior of ridge regression is to drive
  regression coefficients to zero, a state which is a common null
  hypothesis.  The limiting behavior of the hybrid bootstrap is to
  make the class (or continuous target) statistically independent of
  the regressors, as in a permutation test.  Perhaps the hybrid
  bootstrap forces models to possess a weaker dependence between
  predictor variables and the quantity being predicted than they
  otherwise would.  We recognize this is a vague explanation (and
  could be said of other forms of regularization), but we do find that
  the hybrid bootstrap has a lot of practical utility.

* Miscellaneous Acknowledgments
  While we were writing this paper, Michael Jahrer independently used
  the basic hybrid bootstrap as input noise (under the alias ``swap
  noise'') for denoising autoencoders as a component of his winning
  submission to the Porto Seguro Safe Driver Prediction Kaggle
  competition.  Clearly this further establishes the utility of the
  hybrid bootstrap!

  We have also recently learned that there are currently at least
  three distinct groups that have papers at various points in the
  publishing process concerning convex combinations of training
  points, which are similar to hybrid bootstrap combinations
  cite:convex1,convex2,convex3.


@@latex:\printglossaries@@ bibliographystyle:unsrt
bibliography:refs

